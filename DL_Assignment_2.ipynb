{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_Assignment_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharifkaiser/DL_colab/blob/master/DL_Assignment_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z5VOJA5iflLo"
      },
      "source": [
        "# DL Assignment 2 - Adversarial Attacks, Fully Convolutional Networks, Class Activation Mapping\n",
        "\n",
        "Welcome to the second assignment of the DL course 2020! In the last labs we were discussing the task of image classification. You implemented neural networks in Numpy, designed and trained a ConvNet from scratch, added regularization techniques, and used pre-trained models for transfer learning and fine-tuning. Next, you explored different ways to visualize what a ConvNet has learned and which information it consumes in order to make a prediction.\n",
        "\n",
        "In **Assignment 2**, you will use an approach to trick a ConvNet into making wrong predictions, i.e., so called **adversarial attacks**.\n",
        "Furthermore, you will go beyond image classification and implement different approaches to also **localize** an object, e.g., a flower ;), in an image. \n",
        "Considering the presence of more than one object, you will implement different approaches for **object detection**.\n",
        "\n",
        "**After completing this assignment, you will have**\n",
        "- learned how to attack neural networks by **adversarial examples**.\n",
        "- implemented two **sliding window** approaches for object detection.\n",
        "- learned how to transform networks into **fully convolutional networks**.\n",
        "- learned how to interprete ConvNets using **class activation mapping**.\n",
        "- used **YOLOv3** for object detection.\n",
        "\n",
        "**Instructions**\n",
        "- You'll be using Python 3 in the iPython based Google Colaboratory\n",
        "- Lines encapsulated in \"<font color='green'>`### START YOUR CODE HERE ###`</font>\" and \"<font color='green'>`### END YOUR CODE HERE ###`</font>\" denote the code fragments to be completed by you.\n",
        "- There's no need to write any other code.\n",
        "- After writing your code, you can run the cell by either pressing `SHIFT`+`ENTER` or by clicking on the play symbol on the left side of the cell.\n",
        "- We may specify \"<font color='green'>`(≈ X LOC)`</font>\" to tell you about how many lines of code you need to write. This is just a rough estimate, so don't feel bad if your code is longer or shorter.\n",
        "\n",
        "\n",
        "**Much success!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ny9Xj-tjhNw_"
      },
      "source": [
        "<font color='darkblue'>\n",
        "  \n",
        "**Remember**  \n",
        "- Run your cells using `SHIFT`+`ENTER` (or \"Run cell\")\n",
        "- Write code in the designated areas using Python 3 only\n",
        "- Do not modify the code outside of the designated areas\n",
        "- Do not import/use any other packages. Code relying on packages other than the provided ones won't be graded.\n",
        "- Activate GPU acceleration by clicking `Runtime` -> `Change runtime type` and select `GPU` from the dropdown menu entitled `Hardware accelerator`\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O7mgakvlhzBd"
      },
      "source": [
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RM6SHgknVSFz"
      },
      "source": [
        "# 0 - Test for GPU\n",
        "\n",
        "Execute the code below for printing the TF version and testing for GPU availability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ubwQeBgCR6s",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1a3854e-0cb5-408e-ab46-ef7a7ecffef1"
      },
      "source": [
        "#@title Print TF version and GPU stats\n",
        "import tensorflow as tf\n",
        "import sys\n",
        "print('TensorFlow version:', tf.__version__)\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "   raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name), '', sep='\\n')\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.3.0\n",
            "Found GPU at: /device:GPU:0\n",
            "\n",
            "Fri Nov 27 15:39:02 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.38       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   41C    P0    35W / 250W |    359MiB / 16280MiB |      2%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S7fhJrSmo-Ni"
      },
      "source": [
        "#@title Download and import `utils` for Assignment 2\n",
        "\n",
        "import requests, os, zipfile\n",
        "import numpy as np\n",
        "\n",
        "def download_file_from_google_drive(id, destination):\n",
        "    URL = \"https://docs.google.com/uc?export=download\"\n",
        "    session = requests.Session()\n",
        "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
        "    token = get_confirm_token(response)\n",
        "    if token:\n",
        "        params = { 'id' : id, 'confirm' : token }\n",
        "        response = session.get(URL, params = params, stream = True)\n",
        "    save_response_content(response, destination)    \n",
        "\n",
        "def get_confirm_token(response):\n",
        "    for key, value in response.cookies.items():\n",
        "        if key.startswith('download_warning'):\n",
        "            return value\n",
        "    return None\n",
        "\n",
        "def save_response_content(response, destination):\n",
        "    CHUNK_SIZE = 32768\n",
        "    with open(destination, \"wb\") as f:\n",
        "        for chunk in response.iter_content(CHUNK_SIZE):\n",
        "            if chunk: # filter out keep-alive new chunks\n",
        "                f.write(chunk)\n",
        "\n",
        "# download utils\n",
        "download_file_from_google_drive('1WYTduftimsNEPmy9yTRU5YEIRywZezW1', '/content/utils.py')\n",
        "download_file_from_google_drive('1A9bb4igGvgQHmizGUQevSEwsO_jR_sDn', '/content/testimage_detection.jpg')\n",
        "\n",
        "import utils"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h9fo1qGrT9_w"
      },
      "source": [
        "# Part 1 - Adversarial Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "685vmsMDC_Rp"
      },
      "source": [
        "In DL Lab 2.2, we discussed **activation maximization**, i.e., updating input image pixels using gradient ascent in order to maximize the activation of a specific convolution filter or neuron. This way, you created somewhat convoluted visualizations of what your ConvNet was actually looking for in images.\n",
        "\n",
        "You can use a similar idea, i.e., learn how to adjust the input of any classifier in order to enforce a specific output, for attacking a neural network and fooling it into making wrong predictions. The core idea of such carefully designed **adversarial attacks** was first proposed by [Ian Goodfellow et al. in 2014](https://arxiv.org/abs/1412.6572) and represents the first part of Assignment 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fcpaNsVsw16B"
      },
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras.regularizers import l2\n",
        "from tensorflow.keras.utils import get_custom_objects, plot_model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "from scipy.ndimage import zoom\n",
        "from skimage import filters\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xbHiZi0KxQJj"
      },
      "source": [
        "The dataset is provided via `utils.load_dataset()`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hPzjeTUXxKtJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80ae0eac-f5a9-4051-a50b-3b9db601ff99"
      },
      "source": [
        "train_generator, validation_generator, train_steps, validation_steps, num_classes = utils.load_dataset('flowers10')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "downloading file to /tmp/dataset.zip\n",
            "unzipping to  /tmp\n",
            "Found 800 images belonging to 10 classes.\n",
            "Found 200 images belonging to 10 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWSpVf_0Wa4u"
      },
      "source": [
        "## 1.1 - Prepare Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pllPiHosfIy8"
      },
      "source": [
        "You will attack the VGG16 model that you already know from DL Lab 2.2. However, you will top layers that are slightly different compared to the version from DL Lab 2.2. In detail:\n",
        "- Add a [`Flatten` layer](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten) on top of the output of the pre-trained VGG16 (without top layers).\n",
        "- Add a dense layer with 1024 neurons, ReLU activation, and 20% dropout rate.\n",
        "- Add a dense layer with `num_classes` neurons with softmax activation for classification. The output of this last dense layer shall be output of your model.\n",
        "\n",
        "**Task**: Complete the code below to define the architecture and compile the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0ets-2OxE2A"
      },
      "source": [
        "# GRADED FUNCTION: build_from_pretrained (2.5 points)\n",
        "def build_from_pretrained( num_classes, input_shape=(150,150,3), init_lr=1e-4 ):\n",
        "\n",
        "  pre_trained_model = VGG16(input_shape=input_shape,\n",
        "                            weights='imagenet',\n",
        "                            include_top=False)\n",
        "  \n",
        "  ### START YOUR CODE HERE ### (≈ 5 LOC)\n",
        "  model = model.add(Flatten())\n",
        "  layers.flatten()\n",
        "    \n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  # Compile the model\n",
        "  model.compile(loss='categorical_crossentropy',\n",
        "                optimizer=Adam(lr=init_lr),\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "  return model, pre_trained_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAmLGuJNd_dp"
      },
      "source": [
        "Create an instance of the pretrained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "71Wj7NdaxhJ6"
      },
      "source": [
        "classifier_model, pre_trained_model = build_from_pretrained(num_classes)\n",
        "print(classifier_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "flPJccFFyRK6"
      },
      "source": [
        "The cell below downloads weights from a provided checkpoint to the path `ckp_path`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMomhv22x4M0",
        "cellView": "form"
      },
      "source": [
        "#@title Download checkpoint model weights\n",
        "\n",
        "ckp_path = '/tmp/checkpoints/fi_flowers_vgg16_fine_tuned'\n",
        "os.makedirs(ckp_path, exist_ok=True)\n",
        "\n",
        "while not os.path.isfile(os.path.join(ckp_path, 'checkpoint')):\n",
        "  print('Downloading checkpoint')\n",
        "  # download and unzip\n",
        "  download_file_from_google_drive('1k-6fdJDix-ot3qxRr22p01VkfQUjJUSk', os.path.join(ckp_path, 'model.zip'))\n",
        "  utils.unzip( os.path.join(ckp_path, 'model.zip'), os.path.dirname(ckp_path) )\n",
        "\n",
        "ckp_path = os.path.join(ckp_path, 'model')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBEUE6U8hUW_"
      },
      "source": [
        "**Task**: Complete the function `restore_weights()` for loading the weights from the downloaded checkpoint into your [model](https://www.tensorflow.org/api_docs/python/tf/keras/Model).\n",
        "\n",
        "**Note**: The cell will raise an error if the restored model's architecture is different from your model architecture. In that case correct your architecture for the specifications detailed above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iT30oxL1gW4d"
      },
      "source": [
        "# GRADED FUNCTION: restore_weights (1 point)\n",
        "def restore_weights(model, ckp_path):\n",
        "  ''' Restore all model weights from weights file provided by `ckp_path`.\n",
        "  Weights are loaded based on network topology.\n",
        "  \n",
        "  Arguments\n",
        "  ------------\n",
        "    model : instance of tf.keras.Model\n",
        "    ckp_path : path to weights file\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  model : model with loaded weights\n",
        "  '''\n",
        "\n",
        "  ### START YOUR CODE HERE ### (≈1 LOC)\n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  return model\n",
        "\n",
        "# Load the weights\n",
        "classifier_model = restore_weights(classifier_model, ckp_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lp0W2mOezWi_"
      },
      "source": [
        "Evaluation of the restored model on the validation data should give 97% accuracy:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R17bEeUjzefV"
      },
      "source": [
        "_, accuracy = classifier_model.evaluate(validation_generator)\n",
        "print('\\nAccuracy: {:.2f}%'.format(accuracy*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9vCtId1AmYW4"
      },
      "source": [
        "Let's build a function for printing a model's prediction on a given image.\n",
        "\n",
        "**Task**: Complete the function `print_prediction()`. The function shall print the index of the predicted class along with its softmax probability.\n",
        "\n",
        "**Note**: Check `classifier_model.input_shape` if you are unsure about the input shape."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYAXTxk3jLf2"
      },
      "source": [
        "# GRADED FUNCTION: print_prediction (1 point)\n",
        "def print_prediction(model, img):\n",
        "  ''' Compute model prediction for input `img` and print index and probability for predicted class.\n",
        "    \n",
        "  Arguments\n",
        "  ------------\n",
        "    model : classification model\n",
        "    img : input image\n",
        "  '''\n",
        "  \n",
        "  ### START YOUR CODE HERE ### (≈2 LOC)\n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  print('Prediction: Class {} ({:.2f}%)'.format(predicted_class_idx, probabilities[predicted_class_idx]*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbwzE4wfWi6Y"
      },
      "source": [
        "## 1.2 - Random Noise Attacks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y4K4Rrs40V4-"
      },
      "source": [
        "You are now ready to begin some **attacks**! Let's see if some normally distributed random noise fools the classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1-o3hMK3F2K"
      },
      "source": [
        "def add_random_noise(img):\n",
        "  noise = np.random.normal(loc=0.0, scale=0.2, size=img.shape)\n",
        "  return np.clip(img + noise, 0., 1.)\n",
        "\n",
        "def eval_random_noise_attack(img, model):\n",
        "  utils.show(img, figsize=(5,5))\n",
        "  print_prediction(model, img)\n",
        "  \n",
        "  # Generate normally distributed noise\n",
        "  img = add_random_noise(img)\n",
        "\n",
        "  utils.show(img, figsize=(5,5))\n",
        "  print_prediction(model, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PO5OUQSN6S1_"
      },
      "source": [
        "# Get a new image from the validation set\n",
        "img, onehot_class = [items[-1] for items in validation_generator.next()]\n",
        "\n",
        "# Evaluate the attack\n",
        "eval_random_noise_attack(img, classifier_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCC0ApCZ6sIW"
      },
      "source": [
        "Repeat the attack a few times. Even if the images appear pretty perturbed to us, the classifier seems to be rather resilient against such simple attacks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZaWQFWkWopj"
      },
      "source": [
        "## 1.3 - Adversarial Attacks\n",
        "\n",
        "Let's see if you can modify the same image to be an **adversarial example**. To do so, you will create a new model called `adversarial_model`. This model shall learn to modify the input image before feeding it through the `classifier_model`. The adversarial model basically consists of one dense layer. This layer learns additive input patterns that cause the original classification model to predict a certain class predefined by us. In order to add the adversarial patterns, the output size of the dense layer needs to match the number of pixels of the input image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQLUiumND4mM"
      },
      "source": [
        "The architecture of the model can be plotted using the `plot_model` method in the `keras.utils` module. You should see two inputs, `unity` and `original_image`. The `activation` should then be fed into the original classifier model. **Make sure the classifier model has no trainable parameters.** The adversarial model's architecture shall look like this:\n",
        "<figure>\n",
        "<center>\n",
        "<img src='https://www.tu-ilmenau.de/fileadmin/media/SECSY/storage/adv_model.png' width='360' />\n",
        "<figcaption>Expected model architecture</figcaption></center>\n",
        "</figure>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McdQmsRiN3_k"
      },
      "source": [
        "**Task**: Complete the function `build_adversarial_model()` to define the architecture and compile the adversarial model. \n",
        "The model to be attacked, i.e., the original model, is provided as `classifier_model` argument.\n",
        "\n",
        "**Note**: In order to make this idea work, the original model must be prevented from learning anything. Otherwise it will learn to mitigate the modifications introduced by the adversarial model.\n",
        "\n",
        "**Note$^2$**: Check the documentation of the [tf.keras.layers](https://www.tensorflow.org/api_docs/python/tf/keras/layers) module for layers like `Reshape` and `Add`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A93YtBqa_Avs"
      },
      "source": [
        "# GRADED FUNCTION: build_adversarial_model (3.5 points)\n",
        "def build_adversarial_model(img, classifier_model, learning_rate=.01):\n",
        "\n",
        "  # Add `clip` to dictionary of custom objects\n",
        "  get_custom_objects().update({'clip': layers.Activation(clip)})\n",
        "\n",
        "  ### START YOUR CODE HERE ### (7 LOC)\n",
        "\n",
        "  # Freeze the classifier_model\n",
        "\n",
        "\n",
        "  # Input layer for the image fed into adversarial model\n",
        "  image = layers.Input(name='original_image',\n",
        "                       shape=\n",
        "                       )\n",
        "  \n",
        "  # Another input for the adversarial noise\n",
        "  one = layers.Input(shape=(1,), name='unity')\n",
        "\n",
        "  # Dense layer for learning the adversarial noise\n",
        "  noise = layers.Dense( ,\n",
        "                       activation=None,\n",
        "                       use_bias=False,\n",
        "                       kernel_initializer='random_normal',\n",
        "                       kernel_regularizer=l2(.1),\n",
        "                       name='adversarial_noise'\n",
        "                       )(one)\n",
        "\n",
        "  # Reshape the noise to the shape of image\n",
        "  \n",
        "\n",
        "  # Add noise to image to create adversarial image\n",
        "\n",
        "\n",
        "  # Clip values of adversarial image to [0, 1]\n",
        "  x = layers.Activation('clip')(x)\n",
        "\n",
        "  # Feed adversarial image to original classifier model\n",
        "\n",
        "\n",
        "  # Define the adversarial model\n",
        "  adversarial_model = Model(inputs=[image, one],\n",
        "                            outputs=\n",
        "                            )\n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  # Compile the adversarial model\n",
        "  adversarial_model.compile(loss='categorical_crossentropy',\n",
        "                            optimizer=Adam(lr=learning_rate),\n",
        "                            metrics=['accuracy'])\n",
        "\n",
        "  return adversarial_model\n",
        "\n",
        "def clip(x):\n",
        "  '''Custom activation function for clipping adversarial pixel values.'''\n",
        "  return K.clip(x, 0., 1.)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voo_kEC1LkVt"
      },
      "source": [
        "adversarial_model = build_adversarial_model(img, classifier_model)\n",
        "print(\"Classifier model's trainable parameters:\",  len(classifier_model.trainable_weights))\n",
        "plot_model(adversarial_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqwYUaXOc9Oz"
      },
      "source": [
        "print(adversarial_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjyMa4mnELof"
      },
      "source": [
        "Next, you tell adversarial model to modify the input image so that the classification model gets tricked into predicting the class defined by `target_class_idx`. \n",
        "\n",
        "**Task**: Complete the function `create_target_output()` returning a one-hot encoded output vector of length `num_classes` that defines the adversarial output probability distribution. The elements in `target_vector` shall be zero everywhere except for the element at index `target_class_idx`, which shall be `1`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TNVVYE2BNIoT"
      },
      "source": [
        "# GRADED FUNCTION: create_target_output (1 point)\n",
        "def create_target_output(target_class_idx, num_classes):\n",
        "  ''' Create one-hot encoded class vector as target probability vector.\n",
        "  Elements of the returned vector are `1` at index `target_class_idx`\n",
        "  and 0 everywhere else.\n",
        "  \n",
        "  Arguments\n",
        "  ------------\n",
        "    target_class_idx : index of target class\n",
        "    num_classes : number of classes (length of returned vector)\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  target_vector : one-hot encoded class vector\n",
        "  '''\n",
        "\n",
        "  ### START YOUR CODE HERE ### (≈ 2 LOC)\n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  return target_vector"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dUmSIJCcDArG"
      },
      "source": [
        "print(create_target_output(3, 10))\n",
        "# expected output: [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmWs1ISwzsXC"
      },
      "source": [
        "Next, we create some helper functions to actually train and evaluate the adversarial model for an input model and image."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Smk_ba-nzqLP"
      },
      "source": [
        "def add_adversarial_noise(img, classifier_model, target_class_idx, num_classes):\n",
        "\n",
        "  adversarial_model = build_adversarial_model(img, classifier_model)\n",
        "  target_vector = create_target_output(target_class_idx, num_classes)\n",
        "  \n",
        "  # add early stopping to interrupt training if loss stops decreasing\n",
        "  early_stop = EarlyStopping(monitor='loss', \n",
        "                             patience=10,\n",
        "                             verbose=0,\n",
        "                             restore_best_weights=True)\n",
        "\n",
        "  adversarial_model.fit(x={'original_image':np.expand_dims(img, 0), 'unity':np.ones(shape=(1, 1))},\n",
        "                        y=target_vector.reshape(1,-1),\n",
        "                        epochs=100,\n",
        "                        verbose=0,\n",
        "                        callbacks=[early_stop])\n",
        "\n",
        "  quantized_weights = np.round( adversarial_model.get_weights()[0].reshape(img.shape) * 255.) / 255.\n",
        "\n",
        "  # add trained weights to original image and clip values to produce adversarial image\n",
        "  adversarial_img = np.clip(img + quantized_weights, 0., 1.)\n",
        "  \n",
        "  return adversarial_img, quantized_weights\n",
        "\n",
        "def eval_adversarial_noise_attack(img, classifier_model, target_class_idx, num_classes):\n",
        "\n",
        "  utils.show(img, figsize=(5,5))\n",
        "  print_prediction(classifier_model, img)\n",
        "  \n",
        "  # Generate adversarial noise\n",
        "  img, adversarial_noise = add_adversarial_noise(img, classifier_model, target_class_idx, num_classes)\n",
        "\n",
        "  utils.show(img, figsize=(5,5))\n",
        "  print_prediction(classifier_model, img)\n",
        "\n",
        "  adversarial_noise -= np.min(adversarial_noise)\n",
        "  adversarial_noise /= np.max(adversarial_noise)\n",
        "  utils.show(adversarial_noise, figsize=(5,5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DjRLaMJz8vR"
      },
      "source": [
        "Let's see if your adversarial model successfully attacks the `classifier_model`.\n",
        "\n",
        "You may further test more images by executing \n",
        "\n",
        "`img, onehot_class = [items[-1] for items in validation_generator.next()]` \n",
        "\n",
        "prior to evaluation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aqbVHxPREEe"
      },
      "source": [
        "eval_adversarial_noise_attack(img, \n",
        "                              classifier_model, \n",
        "                              # define target class index or select random:\n",
        "                              np.random.choice(list(validation_generator.class_indices.values()), p=np.logical_not(onehot_class)/(num_classes-1)),\n",
        "                              num_classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lLEUCxikHs37"
      },
      "source": [
        "If you did everything right, you should see three images:\n",
        "1. The original image,\n",
        "2. the adversarial image, and\n",
        "3. the min-max normalized adversarial noise added to the original image.\n",
        "\n",
        "As you see, fooling a classifier by generating adversarial noise is very simple. The adversarial noise is much less visible to us compared to random noise. Yet, those barely visible changes trick the classifier into making wrong predictions!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU6kFgX_Dkfx"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-MX6Rnq-KbMW"
      },
      "source": [
        "# Part 2 - Object Detection"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J916SgtBOD51"
      },
      "source": [
        "Let's consider we want to re-use our neural network classifier for performing detection on novel images, e.g., the following one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qb_sSDe8CK4Y"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "img_path = 'testimage_detection.jpg'\n",
        "\n",
        "test_img = load_img(img_path)\n",
        "print('Image of size', test_img.size)\n",
        "utils.show(test_img, figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3TavkR8FK20"
      },
      "source": [
        "The image has $1067 \\times 1600$ px, hence it is considerably larger compared to the input size of the classifier network. To feed it through the classifier network, we need to resize the image to match the input size, i.e., $150 \\times 150$ px:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gChO4JO6Fs2A"
      },
      "source": [
        "# Resize and normalize the image\n",
        "img = img_to_array(test_img.resize( (150, 150) )) / 255.\n",
        "print_prediction(classifier_model, img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "le_MMSRNPF71"
      },
      "source": [
        "The classifier predicts the image to display \"class 1\" (*Arctium tomentosum*) with 84.8% softmax probability, which is actually wrong. The correct prediction would be \"Class 8\" (*Centaurea scabiosa*), which achieves only 14.9% softmax probability.\n",
        "\n",
        "To get an idea on why the classifier predicts *Arctium tomentosum*, we could analyze **where** the network predicts *Arctium tomentosum* (and, of course, also *Centaurea scabiosa*).\n",
        "\n",
        "The image is considerably larger than the input size of the network. The most simple and straight-forward approach for **localization** or **detection** is to extract a series of patches from a sliding window. We resize every patch to match the input size and eventually feed them through the network to compute predictions for every patch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8i1PDZ8ZCMqA"
      },
      "source": [
        "## 2.1 - Sliding Window"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-7-SAHyGADL"
      },
      "source": [
        "In this approach, we will combine your classifier with some traditional programming. We will split the images into overlapping patches and feed each patch into the network for classification. In order to speed up the computations, we create a batch of patches to be forwarded through the network in parallel.\n",
        "\n",
        "**Task**: Complete the function `eval_crops_batch()` for forwarding a `batch` of images through the network and returning a 3D tensor `pmap` of probabilities. The shape of `pmap` has to be `(grid_shape[0], grid_shape[1], num_classes)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qp80ZInvCquM"
      },
      "source": [
        "# GRADED FUNCTION: eval_crops_batch (1 point)\n",
        "def eval_crops_batch(model, batch, grid_shape):\n",
        "  ''' Forward batch through model and return probability maps for each class.\n",
        "  \n",
        "  Arguments\n",
        "  ------------\n",
        "    model : classification model\n",
        "    batch : minibatch of input samples\n",
        "    grid_shape : (height, width) of output probability map\n",
        "\n",
        "  Returns\n",
        "  ------------\n",
        "  pmap : probability map of shape (height, width, num_classes)\n",
        "  '''\n",
        "\n",
        "  print('Forwarding {} patches'.format(batch.shape[0]))\n",
        "\n",
        "  ### START YOUR CODE HERE ### (≈ 2 LOC)  \n",
        "  \n",
        "  # Forward batch through model to make predictions\n",
        "  \n",
        "  # Reshape flat output probabilities to a xy-grid of probabilities\n",
        "\n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  return pmap"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OqsL_4OaLnhw"
      },
      "source": [
        "Now, let's use the original image and extract patches from a grid of $300 \\times 300$ windows with 75% overlap and feed them through the network. A vector of probabilities is then returned for every cell of the grid, i.e., a probability map `pmap`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "055PtisXT11l"
      },
      "source": [
        "%%time\n",
        "# Resize and normalize the image\n",
        "img = img_to_array(test_img) / 255.\n",
        "\n",
        "# Crop overlapping patches and create batch\n",
        "batch, grid_shape = utils.img_crops_to_batch(img, window_size=300)\n",
        "\n",
        "# Forward batch through network to obtain the probability map\n",
        "pmap = eval_crops_batch(classifier_model, batch, grid_shape)\n",
        "\n",
        "# Plot predicted label map\n",
        "plt.imshow( np.argmax(pmap, axis=2) )\n",
        "plt.axis('Off')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBlz5SSKXdHa"
      },
      "source": [
        "If you did everything correctly, you should see an blockly image in which the color denotes the class index of the predicted class per grid center. I.e., the image coarsely resembles a semantically segmented image.\n",
        "\n",
        "You may observe two facts: \n",
        "1. The network actually predicts the correct \"Class 8\" (*Centaurea scabiosa*) at the location of the blooming flower in the lower part of the image. However, there are predictions for \"Class 1\" (*Arctium tomentosum*) at the locations of non-blooming flowers. \n",
        "2. Amounting to several seconds, the computations took a lot of time.\n",
        "\n",
        "To further confirm the first finding, we may plot the prediction distribution of the class with the *highest average probability* across the image, i.e., the `global_mean`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZifMWqbZOYpn"
      },
      "source": [
        "global_mean = np.nanmean(pmap, axis=(0,1))\n",
        "print('Global mean pooled class_idx: {} ({:.2f}%)'.format(np.argmax(global_mean), np.max(global_mean)*100))\n",
        "\n",
        "display_class_id = 1\n",
        "utils.show_img_heatmap(img, utils.match_size(pmap[:,:,display_class_id], img) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lV8O6h3OX9L"
      },
      "source": [
        "... and the highest probability `global_max` across the entire image along with the probability distribution for *Centaurea scabiosa*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrTrErIIU3av"
      },
      "source": [
        "global_max = np.nanmax(pmap, axis=(0,1))\n",
        "print('Global max pooled class_idx: {} ({:.2f}%)'.format(np.argmax(global_max), np.max(global_max)*100))\n",
        "\n",
        "display_class_id = 8\n",
        "utils.show_img_heatmap(img, utils.match_size(pmap[:,:,display_class_id], img) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GG8Rm5eMch_Z"
      },
      "source": [
        "As expected, the network correctly predicts *Centaurea scabiosa* at the locations of the two blooming flower. However, the non-blooming flowers are misclassified as *Arctium tomentosum*. If you search for images of *Arctium tomentosum* using a search engine of your choice, you will understand why this is the case."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-w_XlFEcn7R"
      },
      "source": [
        "## 2.2 - Fully Convolutional Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2UC_u1ccx6_"
      },
      "source": [
        "In DL Lab 2.3, we discussed how ConvNets containing dense layers for classification can be transformed into Fully Convolutional Networks (FCNs) and how they could be used for simple object detection using a convolutional implementation of sliding windows. This way, a lot of computations can be shared which will considerably speed up the execution.\n",
        "\n",
        "The basic idea of FCNs is to replace all dense layers with convolution layers. If input and output shapes as well as activation functions are kept constant, this replacement does not even affect the underlying mathematical operations. We can even re-use the weights of the dense layers!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIu_4DOWe_YK"
      },
      "source": [
        "print(classifier_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYABZytNfCET"
      },
      "source": [
        "You will use the output of layer `block5_pool (MaxPooling2D)` and stack new convolution layers on top. In order to re-use the weights of the `Dense` layers, you extract their weights by calling the `.get_weights()` method on them. Layer weights can then be set during layer definition by using the `weights` argument. However, the weight arrays need to be reshaped first!\n",
        "\n",
        "**Task**: Complete the function `transform_to_fcn()` for converting the input classifier model `model` into a fully convolutional model.\n",
        "\n",
        "**Hint**: Convolution layer weights have the shape $ f^{(l)} \\times f^{(l)} \\times n_f^{(l-1)} \\times n_f^{(l)}$, where $f$ is the kernel size, $n_f$ the number of channels, and $(l)$ the layer index (check your DL Lab 2.1 notes).\n",
        "\n",
        "**Hint$^2$**: In order to re-use the weights, the kernel shape $ f^{(l)} \\times f^{(l)} \\times n_f^{(l-1)}$ needs to match the output shape of the previous layer $n^{(l-1)} \\times n^{(l-1)} \\times n_f^{(l-1)}$.\n",
        "\n",
        "**Hint$^3$**: The number of filters $n_f^{(l)}$ needs to match the number of outputs of the `Dense` layer to be replaced."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N9svN7BLfdNV"
      },
      "source": [
        "# GRADED FUNCTION: transform_to_fcn() (5 points)\n",
        "def transform_to_fcn( model, input_shape=(150,150,3) ):\n",
        "\n",
        "  # Create an instance of the VGG16 without top\n",
        "  fcn_model = VGG16(input_shape=input_shape,\n",
        "                    include_top=False)\n",
        "  \n",
        "  # Load the model weights into fcn_model\n",
        "  for new_layer, layer in zip(fcn_model.layers[1:], model.layers[1:]):\n",
        "    new_layer.set_weights(layer.get_weights()) \n",
        "\n",
        "  # Store output shape of block5_pool; NOTE: you can get a layer either by name or index\n",
        "  block5_out_shape = model.get_layer(\"block5_pool\").output.shape\n",
        "\n",
        "  # Collect the dense layers\n",
        "  dense_layers = [x for x in classifier_model.layers if 'Dense' in str(x.__class__)]\n",
        "\n",
        "  # Extract weights and bias of the second last dense layer\n",
        "  [dense_2_weights, dense_2_bias] = dense_layers[0].get_weights()\n",
        "\n",
        "  # Extract weights and bias of the last dense layer\n",
        "  [dense_1_weights, dense_1_bias] = dense_layers[1].get_weights()\n",
        "\n",
        "  ### START YOUR CODE HERE ### (5 LOC)\n",
        "\n",
        "  # Get output of the `block5_pool` layer for building a new top\n",
        "  x = \n",
        "\n",
        "  # Reshape the dense layer weights to fit the convolution layer weights shape\n",
        "  dense_2_weights = dense_2_weights.reshape( )\n",
        "\n",
        "  # Add convolution layer replacing the second last dense layer\n",
        "  x = layers.Conv2D(filters=, \n",
        "                    kernel_size=, \n",
        "                    activation=, \n",
        "                    weights=[dense_2_weights, dense_2_bias])(x)\n",
        "\n",
        "  # Reshape the weights to fit the convolution layer weights shape\n",
        "  dense_1_weights = dense_1_weights.reshape( )\n",
        "\n",
        "  # Add convolution layer replacing the last dense layer\n",
        "  fcn_output = layers.Conv2D(filters=,\n",
        "                             kernel_size=,\n",
        "                             activation=,\n",
        "                             weights=[dense_1_weights, dense_1_bias])(x)\n",
        "\n",
        "  ### END YOUR CODE HERE ###\n",
        "  \n",
        "  # Add a GAP layer for global mean pooling across the spatial predictions\n",
        "  mean_probabilities = layers.GlobalAveragePooling2D()(fcn_output)\n",
        "\n",
        "  # Add a GMP layer for global max pooling across the spatial predictions\n",
        "  max_probabilities = layers.GlobalMaxPooling2D()(fcn_output)\n",
        "\n",
        "  # Define the fcn model to output the probability map and the mean- and max-pooled probabilities\n",
        "  fcn_model = Model(inputs=fcn_model.input, \n",
        "                    outputs=[fcn_output, \n",
        "                             mean_probabilities, \n",
        "                             max_probabilities]\n",
        "                    )\n",
        "  \n",
        "  return fcn_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jhIlKkHGPnep"
      },
      "source": [
        "Now, transform the network into a FCN with an `input_shape` of `(448, 448, 3)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdw-GvSTfdNX"
      },
      "source": [
        "fcn_model = transform_to_fcn(classifier_model, input_shape=(448, 448, 3))\n",
        "print(fcn_model.summary())\n",
        "\n",
        "print('\\nFCN output shape:', fcn_model.output_shape[0])\n",
        "# expected output: FCN output shape: (None, 11, 11, 10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpGLSzqxQJE3"
      },
      "source": [
        "Now forward the image through the FCN:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMuXGWPTjqxV"
      },
      "source": [
        "%%time\n",
        "# Resize and normalize the image\n",
        "img = img_to_array(test_img.resize( (448, 448) )) / 255.\n",
        "\n",
        "# Forward image through network\n",
        "pmap, global_mean, global_max = fcn_model.predict(np.expand_dims(img, 0) )\n",
        "\n",
        "# Plot predicted label map\n",
        "plt.imshow(np.argmax(pmap[0], axis=2))\n",
        "plt.axis('Off')\n",
        "plt.colorbar()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la-LZMrtRFfX"
      },
      "source": [
        "The label map should appear comparable to the label map computed by the traditional sliding window approach. Compare the execution times to get a sense of the speed up!\n",
        "\n",
        "Also the probability map for *Arctium tomentosum* should be comparable to the results above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B3Raue6ND_as"
      },
      "source": [
        "print('Global mean pooled class_idx: {} ({:.2f}%)'.format(np.argmax(global_mean), np.max(global_mean)*100))\n",
        "\n",
        "img = img_to_array(test_img)/255.\n",
        "display_class_id = 1\n",
        "utils.show_img_heatmap(img, utils.match_size(pmap[0,:,:,display_class_id], img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOIkchkzR4wu"
      },
      "source": [
        "Same should apply to the probability map for *Centaurea scabiosa*:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjoXdXowlqd4"
      },
      "source": [
        "print('Global max pooled class_idx: {} ({:.2f}%)'.format(np.argmax(global_max), np.max(global_max)*100))\n",
        "\n",
        "display_class_id = 8\n",
        "utils.show_img_heatmap(img, utils.match_size(pmap[0,:,:,display_class_id], img))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tP97Y2JsX_Ty"
      },
      "source": [
        "In contrast to a model containing fully connected layers, the layer weights of a FCN do not depend on the input shape. Hence, you may build a FCN with arbitrary input shape using `None` as placeholder, and call it on images of arbitrary sizes:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GaijTbcYpkI"
      },
      "source": [
        "# FCN with arbitrary input shape\n",
        "fcn_model = transform_to_fcn(classifier_model, input_shape=(None, None, 3))\n",
        "\n",
        "for downscale in np.arange(1,5,1, dtype=int)[::-1]:\n",
        "\n",
        "  # Resize and normalize the image, keep aspect ratio constant\n",
        "  img = img_to_array(test_img.reduce(downscale)) / 255.\n",
        "  print('image shape:', img.shape)\n",
        "\n",
        "  # Forward image through network\n",
        "  pmap, global_mean, global_max = fcn_model.predict(np.expand_dims(img, 0) )\n",
        "\n",
        "  print('Global max pooled class_idx: {} ({:.2f}%)'.format(np.argmax(global_max), np.max(global_max)*100))\n",
        "\n",
        "  display_class_id = 8\n",
        "  utils.show_img_heatmap(img, utils.match_size(pmap[0,:,:,display_class_id], img), figsize=(4,4))\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FIHyjb9qZXPB"
      },
      "source": [
        "Why does the predicted area for class 1 decreases as the image size increases?\n",
        "\n",
        "**Task**: Write down your explanation in the next cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHzLCH8raJyj"
      },
      "source": [
        "# GRADED EXPLANATION: (1 point)\n",
        "\"\"\"Enter your answer here\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lcxb3nY2SH1H"
      },
      "source": [
        "**Congratulations**! By transforming your classifier network into a FCN, you build an efficient model that can be used for detection and further interpretation of the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1tskfol0aqDY"
      },
      "source": [
        "## 2.3 - Class Activation Mapping\n",
        "\n",
        "Another approach for interpreting CNN predictions is to localize the largest activations of a certain class in the last convolutional feature map. By weighted mapping of this activation back to the input image, this simple but powerful method allows for highlighting the image areas most relevant for a class prediction. Hence, it also gives an estimate for objects' locations within an image. The method is termed **class activation mapping** (CAM) and was originally proposed by [Bolei Zhou et al. in 2016](https://www.cv-foundation.org/openaccess/content_cvpr_2016/html/Zhou_Learning_Deep_Features_CVPR_2016_paper.html).\n",
        "\n",
        "The idea is to use a *global average pooling* (GAP) layer to reduce the dimensions $n_h \\times n_w \\times n_f$ of the 3D tensor containing the feature maps by computing the average of each feature map, i.e., reduce the dimensions to $1 \\times 1 \\times n_f$. \n",
        "Recall DL Labs 2.1 and 2.2, where you used a *global max pooling* layer prior to classification. I.e., instead of average feature maps, you computed the maximum value for each feature map in previous DL Labs. \n",
        "\n",
        "For CAM, the intuition behind the GAP layer is that each filter of the last convolution layer acts as a detector for a specific high-level object feature and their activation allow for localization in space. The GAP allows for efficient mapping between these feature maps and the last dense layer for classification.\n",
        "\n",
        "**Task**: Complete the function `build_cam_model()` for making CAM modifications on a VGG16. Stack a GAP layer (provided by [`GlobalAveragePooling2D`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D)) on top of the last convolution layer. Finally, add a dense layer for classification. The number of classes is provided in `num_classes`. Compile the model using categorical crossentropy loss, Adam optimizer with learning rate 0.01, and accuracy as metric.\n",
        "\n",
        "**Note**: The input shape is set to `(224, 224, 3)` for increasing the spatial resolution of the last feature maps to `(14, 14)`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yNV8mvGfw-0R"
      },
      "source": [
        "# GRADED FUNCTION: build_cam_model (3 points)\n",
        "def build_cam_model(num_classes, input_shape=(224, 224, 3), fine_tuned_model=None):\n",
        "\n",
        "  pretrained_model = VGG16(input_shape=input_shape,\n",
        "                           weights='imagenet',\n",
        "                           include_top=False)\n",
        "  \n",
        "  if fine_tuned_model: # re-use the weights of the model we fine-tuned earlier\n",
        "    print('using weights from \"{}\"'.format(fine_tuned_model.name))\n",
        "    for new_layer, layer in zip(pretrained_model.layers[1:], fine_tuned_model.layers[1:]):\n",
        "      new_layer.set_weights(layer.get_weights()) \n",
        "\n",
        "  ### START YOUR CODE HERE ### (≈3 LOC)\n",
        "\n",
        "  # Use the output of the last convolution as input for the GAP\n",
        "  x = \n",
        "\n",
        "  # Add classification layer\n",
        "  output = \n",
        "\n",
        "  # Freeze the pretrained model\n",
        "  \n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  # Define the model\n",
        "  cam_model = Model( pretrained_model.input, output )\n",
        "\n",
        "  # Compile\n",
        "  cam_model.compile(loss='categorical_crossentropy',\n",
        "                    optimizer=Adam(lr=0.01),\n",
        "                    metrics=['accuracy'])\n",
        "\n",
        "  return cam_model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o_58RhHgzSWT"
      },
      "source": [
        "cam_model = build_cam_model(num_classes, fine_tuned_model=classifier_model, input_shape=(224, 224, 3))\n",
        "assert np.array_equal( cam_model.layers[-3].output_shape, \n",
        "                      (None, 14, 14, 512) ), 'Wrong input shape or wrong input provided to the GAP layer'\n",
        "\n",
        "print(cam_model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sT3n8K0v6gLp"
      },
      "source": [
        "Now train the CAM model for 5 epochs using the image generators with updated shapes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BzaQZZTw66yt"
      },
      "source": [
        "train_generator, validation_generator, train_steps, validation_steps, num_classes = utils.load_dataset('flowers10', image_shape=(224,224))\n",
        "\n",
        "cam_hist = cam_model.fit(train_generator,\n",
        "                         steps_per_epoch=train_steps,\n",
        "                         epochs=5,\n",
        "                         validation_data=validation_generator,\n",
        "                         validation_steps=validation_steps,\n",
        "                         verbose=2)\n",
        "\n",
        "utils.plot_history(cam_hist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPFhmdF7rsX-"
      },
      "source": [
        "The trained CAM model with GAP layer and classifier on top can now be used for class localization tasks. For localization, you need the feature maps just before the GAP layer. Every element in the GAP layer's output corresponds to one feature map. The connections between the feature maps and the predicted classes are actually the weights of last dense layer that you have just trained! The features important for prediction of a certain class are given higher weights. In result, to obtain a class activation map, you only need to compute the weighted sum of these feature maps.\n",
        "\n",
        "To summarize, you need your CAM model to output\n",
        "1. the predicted class probabilities,\n",
        "2. the output of the last convolution layer, and\n",
        "3. the weights of the dense layer for classification.\n",
        "\n",
        "Based on your trained CAM model, you define a new model `vis_model` for returning these outputs.\n",
        "\n",
        "**Task**: Complete the function `cam_visualization_model()` for returning the output of the last convolution layer along with the classifiers output."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ynp1LkDWB658"
      },
      "source": [
        "# GRADED FUNCTION: cam_visualization_model (1 point)\n",
        "def cam_visualization_model(cam_model):\n",
        "\n",
        "  # Get the weights of the last fully connected layer\n",
        "  weights,_ = cam_model.layers[-1].get_weights()\n",
        "\n",
        "  # Define the vis_model to output the feature map AND the class probabilities\n",
        "  \n",
        "  ### START YOUR CODE HERE ### (≈ 1 LOC)\n",
        "  vis_model = \n",
        "  \n",
        "  ### END YOUR CODE HERE ###\n",
        "\n",
        "  return vis_model, weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gfd2qLHamk39"
      },
      "source": [
        "The function `compute_cam()` now computes the class activation map of a given image:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SaR2eKrrl_06"
      },
      "source": [
        "def compute_cam(img, vis_model, weights, class_idx=None):\n",
        "  \n",
        "  # Prepend batch dimension to image\n",
        "  img = K.expand_dims(img, 0)\n",
        "  \n",
        "  # Forward image through visualization model\n",
        "  feature_maps, class_predictions = vis_model.predict(img)\n",
        "\n",
        "  # Remove preceding batch dimension\n",
        "  feature_maps = K.squeeze(feature_maps, axis=0)\n",
        "  class_predictions = K.squeeze(class_predictions, axis=0)\n",
        "\n",
        "  if class_idx is None:\n",
        "    # Get index of predicted class\n",
        "    class_idx = np.argmax(class_predictions)\n",
        "  \n",
        "  # Upsample feature maps to original image size\n",
        "  feature_maps = zoom(feature_maps, (16, 16, 1), order=1)\n",
        "  \n",
        "  # Compute weighted sum of activation maps for predicted class\n",
        "  cam = np.dot( feature_maps, weights[:, class_idx] )\n",
        "\n",
        "  cam = np.zeros(dtype=np.float32, shape=feature_maps.shape[0:2])\n",
        "  for i, w in enumerate(weights[:, class_idx]):\n",
        "    cam += w * feature_maps[:, :, i]\n",
        "\n",
        "  return cam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKCNGU6RCzk_"
      },
      "source": [
        "%%time\n",
        "# Initialize an instance of the visualization model\n",
        "vis_model, weights = cam_visualization_model(cam_model)\n",
        "\n",
        "# Resize and normalize the image\n",
        "img = img_to_array(test_img.resize( (224, 224) )) / 255.\n",
        "\n",
        "# Compute the class activation map\n",
        "cam = compute_cam(img, vis_model, weights, class_idx=8)\n",
        "\n",
        "# Plot as heatmap overlaying the original image\n",
        "img = img_to_array(test_img)/255.\n",
        "utils.show_img_heatmap(img, utils.match_size(cam, img) )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYCVXx4dZ_Zy"
      },
      "source": [
        "Such spatially confined activations could be used for computing bounding boxes of the \"detections\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4kojAQiZZ4gF"
      },
      "source": [
        "# Compute bounding box and plot on image\n",
        "bboxes = utils.compute_bboxes(cam)\n",
        "utils.show_img_bboxes(img_to_array(test_img)/255., bboxes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOSu7ce3RJjp"
      },
      "source": [
        "In conclusion, **class activation mapping** can be used not only for interpreting a classifier output, but also for object localizatization by localizing classification-relevant parts of the objects in images. \n",
        "\n",
        "Keep in mind:\n",
        "- Probability maps from FCNs as well as weighted feature maps from CAM have rather coarse spatial resolution and require upsampling to the original input size.\n",
        "- In case of CAM, the binarization of the activation map for segmentation can be prone to errors. Hence, the bounding boxes sometimes don't fit very well.\n",
        "- CAM highlights image regions relevant for classification, not entire objects!\n",
        "\n",
        "However, you could use a method like CAM in order to generate bounding box annotations your images and then train a more sophisticated object detector, such as:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcWvqaqjW4Op"
      },
      "source": [
        "##2.4 - (Optional) YOLO - \"You Only Look Once\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x42awh_3XDYz"
      },
      "source": [
        "In DL Lab 2.3, we discussed a powerful family of models for efficient object detection: **You Only Look Once**, or YOLO, which was originally develoved by [Joseph Redmon et al. in 2015](https://arxiv.org/abs/1506.02640). Since the original publication, it has undergone a series of evolutionary steps. Here, you will be using its third incarnation, i.e., the *YOLOv3*.\n",
        "\n",
        "The basic idea of YOLO is to use a Fully Convolutional Network (FCN) that predicts not only class probabilities per grid cell but also candidate bounding boxes depicting potential detections. Hence, the shape of the prediction kernel is $1 \\times 1 \\times ( N_\\text{anchors} \\times ( 5 + N_\\text{classes} ) )$, where $N_\\text{anchors}$ is the number of bounding boxes per cell and $N_\\text{classes}$ the number of classes. Each predicted bounding box has 5 attributes: the 4 box coordinates (center coordinates, width, height) and one prediction confidence, i.e., a probability of the bounding box containing any object at all."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PeG0A-9sLQTE",
        "cellView": "form"
      },
      "source": [
        "#@title Download YOLO source\n",
        "download_file_from_google_drive('1udmTjGfkzOf4eD_AK7dX8erxpGq7LJLu', '/tmp/yolo.zip')\n",
        "utils.unzip('/tmp/yolo.zip', '/content/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y82rBkBHhoFG"
      },
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import Model\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "from yolo import yolo_layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HrvPEVNkcgzs"
      },
      "source": [
        "In this example, you will restore weights of a YOLOv3 trained on the [Open Images v4](https://storage.googleapis.com/openimages/web/factsfigures_v4.html) dataset, which contains 600 classes (including \"plant\"). The weights are provided from [Joseph Redmon's website](https://pjreddie.com/darknet/yolo/) for the original YOLOv3 implementation and converted for Keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l1msZz7I4JlT"
      },
      "source": [
        "ckp_path = os.path.join('yolo', 'checkpoints', 'yolov3.tf')\n",
        "class_names_path = 'yolo/openimages.names'\n",
        "\n",
        "class_names = [c.strip() for c in open(class_names_path).readlines()]\n",
        "num_classes = len(class_names)\n",
        "\n",
        "img_path = 'testimage_detection.jpg'\n",
        "test_img = load_img(img_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4mf39D8MVglT"
      },
      "source": [
        "In *YOLOv3*, detections are computed at three different scales, i.e., using feature maps of three different sizes from three different depths within the network. To capture more fine-grained information, feature maps from earlier layers are concatenated to feature maps of deeper layers ahead of detection.\n",
        "The underlying network (*Darknet-53*) uses 53 convolutional layers with residual skip connections."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yV1dr72GzEqx"
      },
      "source": [
        "def YoloV3(input_shape=(416,416,3), classes=500):\n",
        "\n",
        "  anchors = yolo_layers.yolo_anchors\n",
        "  masks = yolo_layers.yolo_anchor_masks\n",
        "\n",
        "  x = inputs = layers.Input(input_shape)\n",
        "\n",
        "  # Store outputs of layers x_36 and x_61\n",
        "  x_36, x_61, x = yolo_layers.Darknet(name='yolo_darknet')(x)\n",
        "\n",
        "  # Compute predictions at first scale\n",
        "  x = yolo_layers.YoloConv(512, name='yolo_conv_0')(x)\n",
        "  output_0 = yolo_layers.YoloOutput(512, len(masks[0]), classes, name='yolo_output_0')(x)\n",
        "\n",
        "  # Concat output of layer x_61 and compute predictions at second scale\n",
        "  x = yolo_layers.YoloConv(256, name='yolo_conv_1')((x, x_61))\n",
        "  output_1 = yolo_layers.YoloOutput(256, len(masks[1]), classes, name='yolo_output_1')(x)\n",
        "  \n",
        "  # Concat output of layer x_36 and compute predictions at third scale\n",
        "  x = yolo_layers.YoloConv(128, name='yolo_conv_2')((x, x_36))\n",
        "  output_2 = yolo_layers.YoloOutput(128, len(masks[2]), classes, name='yolo_output_2')(x)\n",
        "\n",
        "  # Transform boxes\n",
        "  boxes_0 = layers.Lambda(lambda x: yolo_layers.yolo_boxes(x, anchors[masks[0]], classes),\n",
        "                   name='yolo_boxes_0')(output_0)\n",
        "  boxes_1 = layers.Lambda(lambda x: yolo_layers.yolo_boxes(x, anchors[masks[1]], classes),\n",
        "                   name='yolo_boxes_1')(output_1)\n",
        "  boxes_2 = layers.Lambda(lambda x: yolo_layers.yolo_boxes(x, anchors[masks[2]], classes),\n",
        "                   name='yolo_boxes_2')(output_2)\n",
        "\n",
        "  # Concat boxes from three scales and filter by threshold and non-max suppression\n",
        "  outputs = layers.Lambda(lambda x: yolo_layers.yolo_nms(x, anchors, masks, classes),\n",
        "                   name='yolo_nms')((boxes_0[:3], boxes_1[:3], boxes_2[:3]))\n",
        "\n",
        "  return Model(inputs, outputs, name='yolov3')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Q9B928yzFPT"
      },
      "source": [
        "# Initialize an instance of YOLO\n",
        "yolo = YoloV3(classes=num_classes)\n",
        "\n",
        "# Restore weights from network pre-trained on OpenImages\n",
        "yolo.load_weights(ckp_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yj85FRXspD62"
      },
      "source": [
        "%%time\n",
        "img = img_to_array(test_img.resize( (416, 416) )) / 255.\n",
        "batch = tf.expand_dims(img, axis=0)\n",
        "\n",
        "boxes, scores, classes, nums = yolo(batch)\n",
        "\n",
        "bboxes = np.array([x for x in boxes[0] if not all(x==0)])\n",
        "labels = list()\n",
        "for idx,bbox in enumerate(bboxes):\n",
        "  bbox[2] -= bbox[0]\n",
        "  bbox[3] -= bbox[1]\n",
        "  labels.append('{} ({:.1f}%)'.format(\n",
        "      class_names[int(classes[0][idx])],\n",
        "      scores[0][idx]*100))\n",
        "\n",
        "utils.show_img_bboxes(img_to_array(test_img)/255., bboxes, labels=labels, figsize=(14,14))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m_8KIk_fK2El"
      },
      "source": [
        "---\n",
        "\n",
        "# Congratulations on completing Assignment 2!\n",
        "\n",
        "Complete the steps below for submission.\n",
        "\n",
        "# Submission Instructions\n",
        "\n",
        "You may now submit your notebook to moodle:\n",
        "- Save the notebook (`CTRL`+ `s` or '*File*' -> '*Save*')\n",
        "- Click on '*File*' -> '*Download .ipynb*' for downloading the notebook as IPython Notebook file.\n",
        "- Upload the downloaded IPython Notebook file to **Moodle**."
      ]
    }
  ]
}