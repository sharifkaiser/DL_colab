{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COCO_Image_Viewer.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOHiWcuLs+Gv89Q5hWYrz6j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharifkaiser/DL_colab/blob/master/COCO_Image_Viewer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "snipinaosC3V",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8be574f1-6f34-479a-85e8-6b264132d137"
      },
      "source": [
        "# import tensorflow 2.x\n",
        "try:\n",
        "  # Use the %tensorflow_version magic if in colab.\n",
        "  %tensorflow_version 2.x\n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UXu_01JW5LhU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "142dde6f-f2ef-404d-e917-902370c81c5d"
      },
      "source": [
        "!pip install tensorflow-object-detection-api"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-object-detection-api in /usr/local/lib/python3.6/dist-packages (0.1.1)\n",
            "Requirement already satisfied: tensorflow in /tensorflow-2.1.0/python3.6 (from tensorflow-object-detection-api) (2.1.0)\n",
            "Requirement already satisfied: Pillow>=1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (6.2.2)\n",
            "Requirement already satisfied: wheel in /tensorflow-2.1.0/python3.6 (from tensorflow-object-detection-api) (0.34.2)\n",
            "Requirement already satisfied: twine in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (3.1.1)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (0.5.5)\n",
            "Requirement already satisfied: Protobuf in /tensorflow-2.1.0/python3.6 (from tensorflow-object-detection-api) (3.11.3)\n",
            "Requirement already satisfied: Cython>=0.28.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (0.29.14)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (4.2.6)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (3.1.3)\n",
            "Requirement already satisfied: jupyter in /usr/local/lib/python3.6/dist-packages (from tensorflow-object-detection-api) (1.0.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (0.1.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.1.0 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (1.26.0)\n",
            "Requirement already satisfied: scipy==1.4.1; python_version >= \"3\" in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (1.4.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (1.18.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (3.1.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (0.9.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (1.0.8)\n",
            "Requirement already satisfied: tensorboard<2.2.0,>=2.1.0 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (2.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (1.14.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (0.8.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (0.2.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.2.0,>=2.1.0rc0 in /tensorflow-2.1.0/python3.6 (from tensorflow->tensorflow-object-detection-api) (2.1.0)\n",
            "Requirement already satisfied: tqdm>=4.14 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (4.28.1)\n",
            "Requirement already satisfied: keyring>=15.1 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (21.1.0)\n",
            "Requirement already satisfied: readme-renderer>=21.0 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (24.0)\n",
            "Requirement already satisfied: pkginfo>=1.4.2 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (1.5.0.1)\n",
            "Requirement already satisfied: requests>=2.20 in /tensorflow-2.1.0/python3.6 (from twine->tensorflow-object-detection-api) (2.22.0)\n",
            "Requirement already satisfied: requests-toolbelt!=0.9.0,>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (0.9.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from twine->tensorflow-object-detection-api) (1.5.0)\n",
            "Requirement already satisfied: setuptools>=0.7.0 in /tensorflow-2.1.0/python3.6 (from twine->tensorflow-object-detection-api) (45.1.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tensorflow-object-detection-api) (2.4.6)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tensorflow-object-detection-api) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tensorflow-object-detection-api) (2.6.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->tensorflow-object-detection-api) (1.1.0)\n",
            "Requirement already satisfied: jupyter-console in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.2.0)\n",
            "Requirement already satisfied: nbconvert in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.6.1)\n",
            "Requirement already satisfied: ipywidgets in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (7.5.1)\n",
            "Requirement already satisfied: qtconsole in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (4.6.0)\n",
            "Requirement already satisfied: notebook in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (5.2.2)\n",
            "Requirement already satisfied: ipykernel in /usr/local/lib/python3.6/dist-packages (from jupyter->tensorflow-object-detection-api) (4.6.1)\n",
            "Requirement already satisfied: h5py in /tensorflow-2.1.0/python3.6 (from keras-applications>=1.0.8->tensorflow->tensorflow-object-detection-api) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (1.0.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (0.4.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (3.2)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /tensorflow-2.1.0/python3.6 (from tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (1.11.0)\n",
            "Requirement already satisfied: jeepney>=0.4.2; sys_platform == \"linux\" in /usr/local/lib/python3.6/dist-packages (from keyring>=15.1->twine->tensorflow-object-detection-api) (0.4.2)\n",
            "Requirement already satisfied: SecretStorage>=3; sys_platform == \"linux\" in /usr/local/lib/python3.6/dist-packages (from keyring>=15.1->twine->tensorflow-object-detection-api) (3.1.2)\n",
            "Requirement already satisfied: docutils>=0.13.1 in /usr/local/lib/python3.6/dist-packages (from readme-renderer>=21.0->twine->tensorflow-object-detection-api) (0.15.2)\n",
            "Requirement already satisfied: Pygments in /usr/local/lib/python3.6/dist-packages (from readme-renderer>=21.0->twine->tensorflow-object-detection-api) (2.1.3)\n",
            "Requirement already satisfied: bleach>=2.1.0 in /usr/local/lib/python3.6/dist-packages (from readme-renderer>=21.0->twine->tensorflow-object-detection-api) (3.1.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /tensorflow-2.1.0/python3.6 (from requests>=2.20->twine->tensorflow-object-detection-api) (1.25.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /tensorflow-2.1.0/python3.6 (from requests>=2.20->twine->tensorflow-object-detection-api) (2019.11.28)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /tensorflow-2.1.0/python3.6 (from requests>=2.20->twine->tensorflow-object-detection-api) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /tensorflow-2.1.0/python3.6 (from requests>=2.20->twine->tensorflow-object-detection-api) (3.0.4)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->twine->tensorflow-object-detection-api) (2.1.0)\n",
            "Requirement already satisfied: prompt-toolkit<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (1.0.18)\n",
            "Requirement already satisfied: ipython in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (5.5.0)\n",
            "Requirement already satisfied: jupyter-client in /usr/local/lib/python3.6/dist-packages (from jupyter-console->jupyter->tensorflow-object-detection-api) (5.3.4)\n",
            "Requirement already satisfied: jupyter-core in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (4.6.1)\n",
            "Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.3)\n",
            "Requirement already satisfied: jinja2>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (2.11.1)\n",
            "Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.8.4)\n",
            "Requirement already satisfied: nbformat>=4.4 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (5.0.4)\n",
            "Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (1.4.2)\n",
            "Requirement already satisfied: testpath in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.4.4)\n",
            "Requirement already satisfied: traitlets>=4.2 in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (4.3.3)\n",
            "Requirement already satisfied: defusedxml in /usr/local/lib/python3.6/dist-packages (from nbconvert->jupyter->tensorflow-object-detection-api) (0.6.0)\n",
            "Requirement already satisfied: widgetsnbextension~=3.5.0 in /usr/local/lib/python3.6/dist-packages (from ipywidgets->jupyter->tensorflow-object-detection-api) (3.5.1)\n",
            "Requirement already satisfied: ipython-genutils in /usr/local/lib/python3.6/dist-packages (from qtconsole->jupyter->tensorflow-object-detection-api) (0.2.0)\n",
            "Requirement already satisfied: terminado>=0.3.3; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (0.8.3)\n",
            "Requirement already satisfied: tornado>=4 in /usr/local/lib/python3.6/dist-packages (from notebook->jupyter->tensorflow-object-detection-api) (4.5.3)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /tensorflow-2.1.0/python3.6 (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (1.3.0)\n",
            "Requirement already satisfied: rsa<4.1,>=3.1.4 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (4.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (4.0.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /tensorflow-2.1.0/python3.6 (from google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (0.2.8)\n",
            "Requirement already satisfied: cryptography in /usr/local/lib/python3.6/dist-packages (from SecretStorage>=3; sys_platform == \"linux\"->keyring>=15.1->twine->tensorflow-object-detection-api) (2.8)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.6/dist-packages (from bleach>=2.1.0->readme-renderer>=21.0->twine->tensorflow-object-detection-api) (0.5.1)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.6/dist-packages (from prompt-toolkit<2.0.0,>=1.0.0->jupyter-console->jupyter->tensorflow-object-detection-api) (0.1.8)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->tensorflow-object-detection-api) (0.7.5)\n",
            "Requirement already satisfied: pexpect; sys_platform != \"win32\" in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->tensorflow-object-detection-api) (4.8.0)\n",
            "Requirement already satisfied: simplegeneric>0.8 in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->tensorflow-object-detection-api) (0.8.1)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.6/dist-packages (from ipython->jupyter-console->jupyter->tensorflow-object-detection-api) (4.4.1)\n",
            "Requirement already satisfied: pyzmq>=13 in /usr/local/lib/python3.6/dist-packages (from jupyter-client->jupyter-console->jupyter->tensorflow-object-detection-api) (17.0.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from jinja2>=2.4->nbconvert->jupyter->tensorflow-object-detection-api) (1.1.1)\n",
            "Requirement already satisfied: jsonschema!=2.5.0,>=2.4 in /usr/local/lib/python3.6/dist-packages (from nbformat>=4.4->nbconvert->jupyter->tensorflow-object-detection-api) (2.6.0)\n",
            "Requirement already satisfied: ptyprocess; os_name != \"nt\" in /usr/local/lib/python3.6/dist-packages (from terminado>=0.3.3; sys_platform != \"win32\"->notebook->jupyter->tensorflow-object-detection-api) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /tensorflow-2.1.0/python3.6 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /tensorflow-2.1.0/python3.6 (from rsa<4.1,>=3.1.4->google-auth<2,>=1.6.3->tensorboard<2.2.0,>=2.1.0->tensorflow->tensorflow-object-detection-api) (0.4.8)\n",
            "Requirement already satisfied: cffi!=1.11.3,>=1.8 in /usr/local/lib/python3.6/dist-packages (from cryptography->SecretStorage>=3; sys_platform == \"linux\"->keyring>=15.1->twine->tensorflow-object-detection-api) (1.13.2)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi!=1.11.3,>=1.8->cryptography->SecretStorage>=3; sys_platform == \"linux\"->keyring>=15.1->twine->tensorflow-object-detection-api) (2.19)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IpG4z9n4fEy"
      },
      "source": [
        "We converted from VOC to coco because mobilenet uses coco format dataset. Now that we have created coco format dataset e.g. the json file, we wanna test it with arbitrary images so that we know it has labelled it correctly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kVO2htKyqoZm"
      },
      "source": [
        "import IPython\n",
        "import os\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from math import trunc\n",
        "from PIL import Image as PILImage\n",
        "from PIL import ImageDraw as PILImageDraw"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XQu_en9XqOQu"
      },
      "source": [
        "# Load the dataset json\n",
        "class CocoDataset():\n",
        "    def __init__(self, annotation_path, image_dir):\n",
        "        self.annotation_path = annotation_path\n",
        "        self.image_dir = image_dir\n",
        "        self.colors = ['blue', 'purple', 'red', 'green', 'orange', 'salmon', 'pink', 'gold',\n",
        "                       'orchid', 'slateblue', 'limegreen', 'seagreen', 'darkgreen', 'olive',\n",
        "                       'teal', 'aquamarine', 'steelblue', 'powderblue', 'dodgerblue', 'navy',\n",
        "                       'magenta', 'sienna', 'maroon']\n",
        "\n",
        "        json_file = open(self.annotation_path)\n",
        "        self.coco = json.load(json_file)\n",
        "        json_file.close()\n",
        "\n",
        "        self.process_info()\n",
        "        self.process_licenses()\n",
        "        self.process_categories()\n",
        "        self.process_images()\n",
        "        self.process_segmentations()\n",
        "\n",
        "    def display_info(self):\n",
        "        print('Dataset Info:')\n",
        "        print('=============')\n",
        "        if self.info is None:\n",
        "            return\n",
        "        for key, item in self.info.items():\n",
        "            print('  {}: {}'.format(key, item))\n",
        "\n",
        "        requirements = [['description', str],\n",
        "                        ['url', str],\n",
        "                        ['version', str],\n",
        "                        ['year', int],\n",
        "                        ['contributor', str],\n",
        "                        ['date_created', str]]\n",
        "        for req, req_type in requirements:\n",
        "            if req not in self.info:\n",
        "                print('ERROR: {} is missing'.format(req))\n",
        "            elif type(self.info[req]) != req_type:\n",
        "                print('ERROR: {} should be type {}'.format(req, str(req_type)))\n",
        "        print('')\n",
        "\n",
        "    def display_licenses(self):\n",
        "        print('Licenses:')\n",
        "        print('=========')\n",
        "\n",
        "        if self.licenses is None:\n",
        "            return\n",
        "        requirements = [['id', int],\n",
        "                        ['url', str],\n",
        "                        ['name', str]]\n",
        "        for license in self.licenses:\n",
        "            for key, item in license.items():\n",
        "                print('  {}: {}'.format(key, item))\n",
        "            for req, req_type in requirements:\n",
        "                if req not in license:\n",
        "                    print('ERROR: {} is missing'.format(req))\n",
        "                elif type(license[req]) != req_type:\n",
        "                    print('ERROR: {} should be type {}'.format(\n",
        "                        req, str(req_type)))\n",
        "            print('')\n",
        "        print('')\n",
        "\n",
        "    def display_categories(self):\n",
        "        print('Categories:')\n",
        "        print('=========')\n",
        "        for sc_key, sc_val in self.super_categories.items():\n",
        "            print('  super_category: {}'.format(sc_key))\n",
        "            for cat_id in sc_val:\n",
        "                print('    id {}: {}'.format(\n",
        "                    cat_id, self.categories[cat_id]['name']))\n",
        "            print('')\n",
        "\n",
        "    def display_image(self, image_id, show_polys=True, show_bbox=True, show_crowds=True, use_url=False):\n",
        "        print('Image:')\n",
        "        print('======')\n",
        "        if image_id == 'random':\n",
        "            image_id = random.choice(list(self.images.keys()))\n",
        "\n",
        "        # Print the image info\n",
        "        image = self.images[image_id]\n",
        "        for key, val in image.items():\n",
        "            print('  {}: {}'.format(key, val))\n",
        "\n",
        "        # Open the image\n",
        "        if use_url:\n",
        "            image_path = image['coco_url']\n",
        "            # image_path = image_dir\n",
        "            response = requests.get(image_path)\n",
        "            image = PILImage.open(BytesIO(response.content))\n",
        "\n",
        "        else:\n",
        "            # image_path = os.path.join(self.image_dir, image['file_name'])\n",
        "            print('image path: ' + image_path)\n",
        "            image = PILImage.open(image_path)\n",
        "\n",
        "        # Calculate the size and adjusted display size\n",
        "        max_width = 600\n",
        "        image_width, image_height = image.size\n",
        "        adjusted_width = min(image_width, max_width)\n",
        "        adjusted_ratio = adjusted_width / image_width\n",
        "        adjusted_height = adjusted_ratio * image_height\n",
        "\n",
        "        # Create list of polygons to be drawn\n",
        "        polygons = {}\n",
        "        bbox_polygons = {}\n",
        "        rle_regions = {}\n",
        "        poly_colors = {}\n",
        "        bbox_categories = {}\n",
        "        print('  segmentations ({}):'.format(\n",
        "            len(self.segmentations[image_id])))\n",
        "        for i, segm in enumerate(self.segmentations[image_id]):\n",
        "            polygons_list = []\n",
        "            if segm['iscrowd'] != 0:\n",
        "                # Gotta decode the RLE\n",
        "                px = 0\n",
        "                x, y = 0, 0\n",
        "                rle_list = []\n",
        "                for j, counts in enumerate(segm['segmentation']['counts']):\n",
        "                    if j % 2 == 0:\n",
        "                        # Empty pixels\n",
        "                        px += counts\n",
        "                    else:\n",
        "                        # Need to draw on these pixels, since we are drawing in vector form,\n",
        "                        # we need to draw horizontal lines on the image\n",
        "                        x_start = trunc(\n",
        "                            trunc(px / image_height) * adjusted_ratio)\n",
        "                        y_start = trunc(px % image_height * adjusted_ratio)\n",
        "                        px += counts\n",
        "                        x_end = trunc(trunc(px / image_height)\n",
        "                                      * adjusted_ratio)\n",
        "                        y_end = trunc(px % image_height * adjusted_ratio)\n",
        "                        if x_end == x_start:\n",
        "                            # This is only on one line\n",
        "                            rle_list.append(\n",
        "                                {'x': x_start, 'y': y_start, 'width': 1, 'height': (y_end - y_start)})\n",
        "                        if x_end > x_start:\n",
        "                            # This spans more than one line\n",
        "                            # Insert top line first\n",
        "                            rle_list.append(\n",
        "                                {'x': x_start, 'y': y_start, 'width': 1, 'height': (image_height - y_start)})\n",
        "\n",
        "                            # Insert middle lines if needed\n",
        "                            lines_spanned = x_end - x_start + 1  # total number of lines spanned\n",
        "                            full_lines_to_insert = lines_spanned - 2\n",
        "                            if full_lines_to_insert > 0:\n",
        "                                full_lines_to_insert = trunc(\n",
        "                                    full_lines_to_insert * adjusted_ratio)\n",
        "                                rle_list.append(\n",
        "                                    {'x': (x_start + 1), 'y': 0, 'width': full_lines_to_insert, 'height': image_height})\n",
        "\n",
        "                            # Insert bottom line\n",
        "                            rle_list.append(\n",
        "                                {'x': x_end, 'y': 0, 'width': 1, 'height': y_end})\n",
        "                if len(rle_list) > 0:\n",
        "                    rle_regions[segm['id']] = rle_list\n",
        "            else:\n",
        "                # Add the polygon segmentation\n",
        "                for segmentation_points in segm['segmentation']:\n",
        "                    segmentation_points = np.multiply(\n",
        "                        segmentation_points, adjusted_ratio).astype(int)\n",
        "                    polygons_list.append(\n",
        "                        str(segmentation_points).lstrip('[').rstrip(']'))\n",
        "            polygons[segm['id']] = polygons_list\n",
        "            if i < len(self.colors):\n",
        "                poly_colors[segm['id']] = self.colors[i]\n",
        "            else:\n",
        "                poly_colors[segm['id']] = 'white'\n",
        "\n",
        "            bbox = segm['bbox']\n",
        "            bbox_points = [bbox[0], bbox[1], bbox[0] + bbox[2], bbox[1],\n",
        "                           bbox[0] + bbox[2], bbox[1] +\n",
        "                           bbox[3], bbox[0], bbox[1] + bbox[3],\n",
        "                           bbox[0], bbox[1]]\n",
        "            bbox_points = np.multiply(bbox_points, adjusted_ratio).astype(int)\n",
        "            bbox_polygons[segm['id']] = str(\n",
        "                bbox_points).lstrip('[').rstrip(']')\n",
        "            bbox_categories[segm['id']] = self.categories[segm['category_id']]\n",
        "            # Print details\n",
        "            print('    {}:{}:{}'.format(\n",
        "                segm['id'], poly_colors[segm['id']], self.categories[segm['category_id']]))\n",
        "\n",
        "        # Draw segmentation polygons on image\n",
        "        html = '<div class=\"container\" style=\"position:relative;\">'\n",
        "        html += '<img src=\"{}\" style=\"position:relative;top:0px;left:0px;width:{}px;\">'.format(\n",
        "            image_path, adjusted_width)\n",
        "        html += '<div class=\"svgclass\"><svg width=\"{}\" height=\"{}\">'.format(\n",
        "            adjusted_width, adjusted_height)\n",
        "\n",
        "        if show_polys:\n",
        "            for seg_id, points_list in polygons.items():\n",
        "                fill_color = poly_colors[seg_id]\n",
        "                stroke_color = poly_colors[seg_id]\n",
        "                for points in points_list:\n",
        "                    html += '<polygon points=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0.5\" />'.format(\n",
        "                        points, fill_color, stroke_color)\n",
        "\n",
        "        if show_crowds:\n",
        "            for seg_id, rect_list in rle_regions.items():\n",
        "                fill_color = poly_colors[seg_id]\n",
        "                stroke_color = poly_colors[seg_id]\n",
        "                for rect_def in rect_list:\n",
        "                    x, y = rect_def['x'], rect_def['y']\n",
        "                    w, h = rect_def['width'], rect_def['height']\n",
        "                    html += '<rect x=\"{}\" y=\"{}\" width=\"{}\" height=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0.5; stroke-opacity:0.5\" />'.format(\n",
        "                        x, y, w, h, fill_color, stroke_color)\n",
        "\n",
        "        if show_bbox:\n",
        "            for seg_id, points in bbox_polygons.items():\n",
        "                x, y = [int(i) for i in points.split()[:2]]\n",
        "                html += '<text x=\"{}\" y=\"{}\" fill=\"yellow\">{}</text>'.format(\n",
        "                    x, y, bbox_categories[seg_id][\"name\"])\n",
        "                fill_color = poly_colors[seg_id]\n",
        "                stroke_color = poly_colors[seg_id]\n",
        "                html += '<polygon points=\"{}\" style=\"fill:{}; stroke:{}; stroke-width:1; fill-opacity:0\" />'.format(\n",
        "                    points, fill_color, stroke_color)\n",
        "\n",
        "        html += '</svg></div>'\n",
        "        html += '</div>'\n",
        "        html += '<style>'\n",
        "        html += '.svgclass { position:absolute; top:0px; left:0px;}'\n",
        "        html += '</style>'\n",
        "        return html\n",
        "\n",
        "    def process_info(self):\n",
        "        self.info = self.coco.get('info')\n",
        "\n",
        "    def process_licenses(self):\n",
        "        self.licenses = self.coco.get('licenses')\n",
        "\n",
        "    def process_categories(self):\n",
        "        self.categories = {}\n",
        "        self.super_categories = {}\n",
        "        for category in self.coco['categories']:\n",
        "            cat_id = category['id']\n",
        "            super_category = category['supercategory']\n",
        "\n",
        "            # Add category to the categories dict\n",
        "            if cat_id not in self.categories:\n",
        "                self.categories[cat_id] = category\n",
        "            else:\n",
        "                print(\"ERROR: Skipping duplicate category id: {}\".format(category))\n",
        "\n",
        "            # Add category to super_categories dict\n",
        "            if super_category not in self.super_categories:\n",
        "                # Create a new set with the category id\n",
        "                self.super_categories[super_category] = {cat_id}\n",
        "            else:\n",
        "                self.super_categories[super_category] |= {\n",
        "                    cat_id}  # Add category id to the set\n",
        "\n",
        "    def process_images(self):\n",
        "        self.images = {}\n",
        "        for image in self.coco['images']:\n",
        "            image_id = image['id']\n",
        "            if image_id in self.images:\n",
        "                print(\"ERROR: Skipping duplicate image id: {}\".format(image))\n",
        "            else:\n",
        "                self.images[image_id] = image\n",
        "\n",
        "    def process_segmentations(self):\n",
        "        self.segmentations = {}\n",
        "        for segmentation in self.coco['annotations']:\n",
        "            image_id = segmentation['image_id']\n",
        "            if image_id not in self.segmentations:\n",
        "                self.segmentations[image_id] = []\n",
        "            self.segmentations[image_id].append(segmentation)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29ugcAV11zaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f75d6cf2-dd40-4d4a-fa5e-b258bdaf4761"
      },
      "source": [
        "# mount google drive in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # this is saved under /content/drive dir"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCzv8IAG2hG1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "6fd45a81-6114-4d96-9468-4092cc683555"
      },
      "source": [
        "annotation_path = '/content/drive/My Drive/board_detection/train.json'\n",
        "image_dir = '/content/drive/My Drive/board_detection/train'\n",
        "\n",
        "coco_dataset = CocoDataset(annotation_path, image_dir)\n",
        "coco_dataset.display_info()\n",
        "coco_dataset.display_licenses()\n",
        "coco_dataset.display_categories()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset Info:\n",
            "=============\n",
            "Licenses:\n",
            "=========\n",
            "Categories:\n",
            "=========\n",
            "  super_category: none\n",
            "    id 0: AvNetMicrozed\n",
            "    id 1: AvNetMicrozed_back\n",
            "    id 2: CSOC-Camera-Mdoule-P1,Rev1\n",
            "    id 3: CSOC-Camera-Mdoule-P1,Rev1_back\n",
            "    id 4: CSOC-Camera-Mdoule-P2,Rev.1\n",
            "    id 5: CSOC-Camera-Mdoule-P2,Rev.1_back\n",
            "    id 6: E627280\n",
            "    id 7: E627280_back\n",
            "    id 8: E627797\n",
            "    id 9: E627797_back\n",
            "    id 10: Raspberry-pi-b+\n",
            "    id 11: Raspberry-pi-b+_back\n",
            "    id 12: ZYBO-Z7\n",
            "    id 13: ZYBO-Z7_back\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3PLXHkY3el6",
        "colab": {
          "resources": {
            "http://localhost:8080/content/drive/My%20Drive/board_detection/train/20200206_172323.jpg": {
              "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
              "ok": false,
              "headers": [
                [
                  "content-length",
                  "1449"
                ],
                [
                  "content-type",
                  "text/html; charset=utf-8"
                ]
              ],
              "status": 404,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 631
        },
        "outputId": "dbe2f1a5-7e79-4060-ffe5-2dba5e5b9abc"
      },
      "source": [
        "html = coco_dataset.display_image(20200206_172323, use_url=False) # first argument is the image name without extension\n",
        "IPython.display.HTML(html)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image:\n",
            "======\n",
            "  file_name: 20200206_172323.jpg\n",
            "  height: 600\n",
            "  width: 800\n",
            "  id: 20200206172323\n",
            "  segmentations (3):\n",
            "    164:blue:{'supercategory': 'none', 'id': 9, 'name': 'E627797_back'}\n",
            "    165:purple:{'supercategory': 'none', 'id': 3, 'name': 'CSOC-Camera-Mdoule-P1,Rev1_back'}\n",
            "    166:red:{'supercategory': 'none', 'id': 4, 'name': 'CSOC-Camera-Mdoule-P2,Rev.1'}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div class=\"container\" style=\"position:relative;\"><img src=\"/content/drive/My Drive/board_detection/train/20200206_172323.jpg\" style=\"position:relative;top:0px;left:0px;width:600px;\"><div class=\"svgclass\"><svg width=\"600\" height=\"450.0\"><text x=\"139\" y=\"66\" fill=\"yellow\">E627797_back</text><polygon points=\"139  66 271  66 271 224 139 224 139  66\" style=\"fill:blue; stroke:blue; stroke-width:1; fill-opacity:0\" /><text x=\"279\" y=\"201\" fill=\"yellow\">CSOC-Camera-Mdoule-P1,Rev1_back</text><polygon points=\"279 201 363 201 363 290 279 290 279 201\" style=\"fill:purple; stroke:purple; stroke-width:1; fill-opacity:0\" /><text x=\"220\" y=\"306\" fill=\"yellow\">CSOC-Camera-Mdoule-P2,Rev.1</text><polygon points=\"220 306 300 306 300 410 220 410 220 306\" style=\"fill:red; stroke:red; stroke-width:1; fill-opacity:0\" /></svg></div></div><style>.svgclass { position:absolute; top:0px; left:0px;}</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRsmi7P4Upu4"
      },
      "source": [
        "# run this code if duplicate flag error occurs\n",
        "def del_all_flags(FLAGS):\n",
        "    flags_dict = FLAGS._flags()    \n",
        "    keys_list = [keys for keys in flags_dict]    \n",
        "    for keys in keys_list:\n",
        "        FLAGS.__delattr__(keys)\n",
        "\n",
        "del_all_flags(tf.flags.FLAGS)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdjYGhDb41RY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "f2ca0a67-83e0-406c-dd91-f7bef5b6ba6e"
      },
      "source": [
        "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n",
        "#\n",
        "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "#     http://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License.\n",
        "# ==============================================================================\n",
        "\n",
        "r\"\"\"Convert raw COCO dataset to TFRecord for object_detection.\n",
        "Please note that this tool creates sharded output files.\n",
        "Example usage:\n",
        "    python create_coco_tf_record.py --logtostderr \\\n",
        "      --train_image_dir=\"${TRAIN_IMAGE_DIR}\" \\\n",
        "      --val_image_dir=\"${VAL_IMAGE_DIR}\" \\\n",
        "      --test_image_dir=\"${TEST_IMAGE_DIR}\" \\\n",
        "      --train_annotations_file=\"${TRAIN_ANNOTATIONS_FILE}\" \\\n",
        "      --val_annotations_file=\"${VAL_ANNOTATIONS_FILE}\" \\\n",
        "      --testdev_annotations_file=\"${TESTDEV_ANNOTATIONS_FILE}\" \\\n",
        "      --output_dir=\"${OUTPUT_DIR}\"\n",
        "\"\"\"\n",
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import hashlib\n",
        "import io\n",
        "import json\n",
        "import os\n",
        "import contextlib2\n",
        "import numpy as np\n",
        "import PIL.Image\n",
        "\n",
        "from pycocotools import mask\n",
        "import tensorflow.compat.v1 as tf\n",
        "\n",
        "from object_detection.dataset_tools import tf_record_creation_util\n",
        "from object_detection.utils import dataset_util\n",
        "from object_detection.utils import label_map_util\n",
        "\n",
        "\n",
        "flags = tf.app.flags\n",
        "tf.flags.DEFINE_string('f', '', 'kernel')\n",
        "tf.flags.DEFINE_boolean('include_masks', False,\n",
        "                        'Whether to include instance segmentations masks ')\n",
        "tf.flags.DEFINE_string('train_image_dir', '/content/drive/My Drive/board_detection/train',\n",
        "                       'Training image directory.')\n",
        "tf.flags.DEFINE_string('val_image_dir', '/content/drive/My Drive/board_detection/validation',\n",
        "                       'Validation image directory.')\n",
        "tf.flags.DEFINE_string('test_image_dir', '/content/drive/My Drive/board_detection/test',\n",
        "                       'Test image directory.')\n",
        "tf.flags.DEFINE_string('train_annotations_file', '/content/drive/My Drive/board_detection/train.json',\n",
        "                       'Training annotations JSON file.')\n",
        "tf.flags.DEFINE_string('val_annotations_file', '/content/drive/My Drive/board_detection/validation.json',\n",
        "                       'Validation annotations JSON file.')\n",
        "tf.flags.DEFINE_string('testdev_annotations_file', '/content/drive/My Drive/board_detection/test.json',\n",
        "                       'Test-dev annotations JSON file.')\n",
        "tf.flags.DEFINE_string('output_dir', '/content/drive/My Drive/board_detection/', 'Output data directory.')\n",
        "\n",
        "FLAGS = flags.FLAGS\n",
        "\n",
        "tf.logging.set_verbosity(tf.logging.INFO)\n",
        "\n",
        "\n",
        "def create_tf_example(image,\n",
        "                      annotations_list,\n",
        "                      image_dir,\n",
        "                      category_index,\n",
        "                      include_masks=False):\n",
        "  \"\"\"Converts image and annotations to a tf.Example proto.\n",
        "  Args:\n",
        "    image: dict with keys:\n",
        "      [u'license', u'file_name', u'coco_url', u'height', u'width',\n",
        "      u'date_captured', u'flickr_url', u'id']\n",
        "    annotations_list:\n",
        "      list of dicts with keys:\n",
        "      [u'segmentation', u'area', u'iscrowd', u'image_id',\n",
        "      u'bbox', u'category_id', u'id']\n",
        "      Notice that bounding box coordinates in the official COCO dataset are\n",
        "      given as [x, y, width, height] tuples using absolute coordinates where\n",
        "      x, y represent the top-left (0-indexed) corner.  This function converts\n",
        "      to the format expected by the Tensorflow Object Detection API (which is\n",
        "      which is [ymin, xmin, ymax, xmax] with coordinates normalized relative\n",
        "      to image size).\n",
        "    image_dir: directory containing the image files.\n",
        "    category_index: a dict containing COCO category information keyed\n",
        "      by the 'id' field of each category.  See the\n",
        "      label_map_util.create_category_index function.\n",
        "    include_masks: Whether to include instance segmentations masks\n",
        "      (PNG encoded) in the result. default: False.\n",
        "  Returns:\n",
        "    example: The converted tf.Example\n",
        "    num_annotations_skipped: Number of (invalid) annotations that were ignored.\n",
        "  Raises:\n",
        "    ValueError: if the image pointed to by data['filename'] is not a valid JPEG\n",
        "  \"\"\"\n",
        "  image_height = image['height']\n",
        "  image_width = image['width']\n",
        "  filename = image['file_name']\n",
        "  image_id = image['id']\n",
        "\n",
        "  full_path = os.path.join(image_dir, filename)\n",
        "  with tf.gfile.GFile(full_path, 'rb') as fid:\n",
        "    encoded_jpg = fid.read()\n",
        "  encoded_jpg_io = io.BytesIO(encoded_jpg)\n",
        "  image = PIL.Image.open(encoded_jpg_io)\n",
        "  key = hashlib.sha256(encoded_jpg).hexdigest()\n",
        "\n",
        "  xmin = []\n",
        "  xmax = []\n",
        "  ymin = []\n",
        "  ymax = []\n",
        "  is_crowd = []\n",
        "  category_names = []\n",
        "  category_ids = []\n",
        "  area = []\n",
        "  encoded_mask_png = []\n",
        "  num_annotations_skipped = 0\n",
        "  for object_annotations in annotations_list:\n",
        "    (x, y, width, height) = tuple(object_annotations['bbox'])\n",
        "    if width <= 0 or height <= 0:\n",
        "      num_annotations_skipped += 1\n",
        "      continue\n",
        "    if x + width > image_width or y + height > image_height:\n",
        "      num_annotations_skipped += 1\n",
        "      continue\n",
        "    xmin.append(float(x) / image_width)\n",
        "    xmax.append(float(x + width) / image_width)\n",
        "    ymin.append(float(y) / image_height)\n",
        "    ymax.append(float(y + height) / image_height)\n",
        "    is_crowd.append(object_annotations['iscrowd'])\n",
        "    category_id = int(object_annotations['category_id'])\n",
        "    category_ids.append(category_id)\n",
        "    category_names.append(category_index[category_id]['name'].encode('utf8'))\n",
        "    area.append(object_annotations['area'])\n",
        "\n",
        "    if include_masks:\n",
        "      run_len_encoding = mask.frPyObjects(object_annotations['segmentation'],\n",
        "                                          image_height, image_width)\n",
        "      binary_mask = mask.decode(run_len_encoding)\n",
        "      if not object_annotations['iscrowd']:\n",
        "        binary_mask = np.amax(binary_mask, axis=2)\n",
        "      pil_image = PIL.Image.fromarray(binary_mask)\n",
        "      output_io = io.BytesIO()\n",
        "      pil_image.save(output_io, format='PNG')\n",
        "      encoded_mask_png.append(output_io.getvalue())\n",
        "  feature_dict = {\n",
        "      'image/height':\n",
        "          dataset_util.int64_feature(image_height),\n",
        "      'image/width':\n",
        "          dataset_util.int64_feature(image_width),\n",
        "      'image/filename':\n",
        "          dataset_util.bytes_feature(filename.encode('utf8')),\n",
        "      'image/source_id':\n",
        "          dataset_util.bytes_feature(str(image_id).encode('utf8')),\n",
        "      'image/key/sha256':\n",
        "          dataset_util.bytes_feature(key.encode('utf8')),\n",
        "      'image/encoded':\n",
        "          dataset_util.bytes_feature(encoded_jpg),\n",
        "      'image/format':\n",
        "          dataset_util.bytes_feature('jpeg'.encode('utf8')),\n",
        "      'image/object/bbox/xmin':\n",
        "          dataset_util.float_list_feature(xmin),\n",
        "      'image/object/bbox/xmax':\n",
        "          dataset_util.float_list_feature(xmax),\n",
        "      'image/object/bbox/ymin':\n",
        "          dataset_util.float_list_feature(ymin),\n",
        "      'image/object/bbox/ymax':\n",
        "          dataset_util.float_list_feature(ymax),\n",
        "      'image/object/class/text':\n",
        "          dataset_util.bytes_list_feature(category_names),\n",
        "      'image/object/is_crowd':\n",
        "          dataset_util.int64_list_feature(is_crowd),\n",
        "      'image/object/area':\n",
        "          dataset_util.float_list_feature(area),\n",
        "  }\n",
        "  if include_masks:\n",
        "    feature_dict['image/object/mask'] = (\n",
        "        dataset_util.bytes_list_feature(encoded_mask_png))\n",
        "  example = tf.train.Example(features=tf.train.Features(feature=feature_dict))\n",
        "  return key, example, num_annotations_skipped\n",
        "\n",
        "\n",
        "def _create_tf_record_from_coco_annotations(\n",
        "    annotations_file, image_dir, output_path, include_masks, num_shards):\n",
        "  \"\"\"Loads COCO annotation json files and converts to tf.Record format.\n",
        "  Args:\n",
        "    annotations_file: JSON file containing bounding box annotations.\n",
        "    image_dir: Directory containing the image files.\n",
        "    output_path: Path to output tf.Record file.\n",
        "    include_masks: Whether to include instance segmentations masks\n",
        "      (PNG encoded) in the result. default: False.\n",
        "    num_shards: number of output file shards.\n",
        "  \"\"\"\n",
        "  with contextlib2.ExitStack() as tf_record_close_stack, \\\n",
        "      tf.gfile.GFile(annotations_file, 'r') as fid:\n",
        "    output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n",
        "        tf_record_close_stack, output_path, num_shards)\n",
        "    groundtruth_data = json.load(fid)\n",
        "    images = groundtruth_data['images']\n",
        "    category_index = label_map_util.create_category_index(\n",
        "        groundtruth_data['categories'])\n",
        "\n",
        "    annotations_index = {}\n",
        "    if 'annotations' in groundtruth_data:\n",
        "      tf.logging.info(\n",
        "          'Found groundtruth annotations. Building annotations index.')\n",
        "      for annotation in groundtruth_data['annotations']:\n",
        "        image_id = annotation['image_id']\n",
        "        if image_id not in annotations_index:\n",
        "          annotations_index[image_id] = []\n",
        "        annotations_index[image_id].append(annotation)\n",
        "    missing_annotation_count = 0\n",
        "    for image in images:\n",
        "      image_id = image['id']\n",
        "      if image_id not in annotations_index:\n",
        "        missing_annotation_count += 1\n",
        "        annotations_index[image_id] = []\n",
        "    tf.logging.info('%d images are missing annotations.',\n",
        "                    missing_annotation_count)\n",
        "\n",
        "    total_num_annotations_skipped = 0\n",
        "    for idx, image in enumerate(images):\n",
        "      if idx % 100 == 0:\n",
        "        tf.logging.info('On image %d of %d', idx, len(images))\n",
        "      annotations_list = annotations_index[image['id']]\n",
        "      _, tf_example, num_annotations_skipped = create_tf_example(\n",
        "          image, annotations_list, image_dir, category_index, include_masks)\n",
        "      total_num_annotations_skipped += num_annotations_skipped\n",
        "      shard_idx = idx % num_shards\n",
        "      output_tfrecords[shard_idx].write(tf_example.SerializeToString())\n",
        "    tf.logging.info('Finished writing, skipped %d annotations.',\n",
        "                    total_num_annotations_skipped)\n",
        "\n",
        "\n",
        "# def main(_):\n",
        "# assert FLAGS.train_image_dir, '/content/drive/My Drive/board_detection/train'\n",
        "# assert FLAGS.val_image_dir, '/content/drive/My Drive/board_detection/validation'\n",
        "# assert FLAGS.test_image_dir, '/content/drive/My Drive/board_detection/test'\n",
        "# assert FLAGS.train_annotations_file, '/content/drive/My Drive/board_detection/train.json'\n",
        "# assert FLAGS.val_annotations_file, '/content/drive/My Drive/board_detection/validation.json'\n",
        "# assert FLAGS.testdev_annotations_file, '/content/drive/My Drive/board_detection/test.json'\n",
        "\n",
        "if not tf.gfile.IsDirectory(FLAGS.output_dir):\n",
        "  tf.gfile.MakeDirs(FLAGS.output_dir)\n",
        "train_output_path = os.path.join(FLAGS.output_dir, 'coco_train.record')\n",
        "val_output_path = os.path.join(FLAGS.output_dir, 'coco_val.record')\n",
        "testdev_output_path = os.path.join(FLAGS.output_dir, 'coco_testdev.record')\n",
        "\n",
        "_create_tf_record_from_coco_annotations(\n",
        "    FLAGS.train_annotations_file,\n",
        "    FLAGS.train_image_dir,\n",
        "    train_output_path,\n",
        "    FLAGS.include_masks,\n",
        "    num_shards=100)\n",
        "_create_tf_record_from_coco_annotations(\n",
        "    FLAGS.val_annotations_file,\n",
        "    FLAGS.val_image_dir,\n",
        "    val_output_path,\n",
        "    FLAGS.include_masks,\n",
        "    num_shards=10)\n",
        "_create_tf_record_from_coco_annotations(\n",
        "    FLAGS.testdev_annotations_file,\n",
        "    FLAGS.test_image_dir,\n",
        "    testdev_output_path,\n",
        "    FLAGS.include_masks,\n",
        "    num_shards=100)\n",
        "\n",
        "\n",
        "# if you find DuplicateFlagError: always run the previous code snippet\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-00bfdba7aa6e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    244\u001b[0m     \u001b[0mtrain_output_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minclude_masks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m     num_shards=100)\n\u001b[0m\u001b[1;32m    247\u001b[0m _create_tf_record_from_coco_annotations(\n\u001b[1;32m    248\u001b[0m     \u001b[0mFLAGS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mval_annotations_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-00bfdba7aa6e>\u001b[0m in \u001b[0;36m_create_tf_record_from_coco_annotations\u001b[0;34m(annotations_file, image_dir, output_path, include_masks, num_shards)\u001b[0m\n\u001b[1;32m    187\u001b[0m   \u001b[0;32mwith\u001b[0m \u001b[0mcontextlib2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExitStack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf_record_close_stack\u001b[0m\u001b[0;34m,\u001b[0m       \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mannotations_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m     output_tfrecords = tf_record_creation_util.open_sharded_output_tfrecords(\n\u001b[0;32m--> 189\u001b[0;31m         tf_record_close_stack, output_path, num_shards)\n\u001b[0m\u001b[1;32m    190\u001b[0m     \u001b[0mgroundtruth_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m     \u001b[0mimages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgroundtruth_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'images'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/object_detection/dataset_tools/tf_record_creation_util.py\u001b[0m in \u001b[0;36mopen_sharded_output_tfrecords\u001b[0;34m(exit_stack, base_path, num_shards)\u001b[0m\n\u001b[1;32m     41\u001b[0m   tfrecords = [\n\u001b[1;32m     42\u001b[0m       \u001b[0mexit_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_record_output_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m   ]\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/object_detection/dataset_tools/tf_record_creation_util.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     41\u001b[0m   tfrecords = [\n\u001b[1;32m     42\u001b[0m       \u001b[0mexit_stack\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menter_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython_io\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTFRecordWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtf_record_output_filenames\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m   ]\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'python_io'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VnR9q6FeVNai"
      },
      "source": [
        "# actually run the previous tfrecord making function\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}