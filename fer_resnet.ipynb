{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "fer_resnet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sharifkaiser/DL_colab/blob/master/fer_resnet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGNpmn43C0O6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "75d8b51d-8520-467c-9c5e-d831b82283d4"
      },
      "source": [
        "!pip3 install kaggle # api for downloading dataset\n",
        "try:\n",
        "  # Use the %tensorflow_version magic if in colab.\n",
        "  %tensorflow_version 1.15  # tf 2.0 does not support quantization aware training \n",
        "except Exception:\n",
        "  pass\n",
        "\n",
        "import tensorflow as tf\n",
        "tf.enable_eager_execution() # this is must for post training quantization\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.6/dist-packages (1.5.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.6/dist-packages (from kaggle) (2019.11.28)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.21.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.28.1)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.6/dist-packages (from kaggle) (4.0.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.12.0)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from kaggle) (2.6.1)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from kaggle) (1.24.3)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (2.8)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->kaggle) (3.0.4)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.6/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "`%tensorflow_version` only switches the major version: `1.x` or `2.x`.\n",
            "You set: `1.15  # tf 2.0 does not support quantization aware training`. This will be interpreted as: `1.x`.\n",
            "\n",
            "\n",
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GsUfMqttg5pW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "7978da29-987f-441b-f966-e29aef77e84a"
      },
      "source": [
        "# strategy: I have uploaded the csv file to google drive, then mounted \n",
        "# drive in colab and then converted csv file into images\n",
        "# mount google drive in colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive') # this is saved under /content/drive dir\n",
        "\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "drive  sample_data\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9rjElDhllVa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "48699dca-6be5-4272-fa81-b8d3eb70f9ec"
      },
      "source": [
        "# convert from csv to image\n",
        "import os\n",
        "import argparse\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "# initialize global variables\n",
        "TRAIN_DIR = ''\n",
        "TEST_DIR = ''\n",
        "\n",
        "# declare dictionary for readability\n",
        "Emotion = {\n",
        "    0: 'angry',\n",
        "    1: 'disgust',\n",
        "    2: 'fear',\n",
        "    3: 'happy',\n",
        "    4: 'sad',\n",
        "    5: 'surprise',\n",
        "    6: 'neutral'\n",
        "}\n",
        "\n",
        "# kaggle facial expression recognition dataset contains 48*48 pixel greyscale images\n",
        "IMG_WIDTH = 48\n",
        "IMG_HEIGHT = 48\n",
        "\n",
        "\n",
        "def csv_to_images(source):\n",
        "\n",
        "    data = pd.read_csv(source)  # read file\n",
        "    # see_data(data)    # see the csv data to get an idea\n",
        "    _make_dir()\n",
        " \n",
        "    for index, row in data.iterrows():\n",
        "        img =_create_img(row['pixels'])\n",
        "\n",
        "        # Save image under training folder\n",
        "        # python does not have switch case, so use if elif !\n",
        "        img_path = get_img_dir(int(row['emotion']), row['Usage'], index)\n",
        "        img.save(img_path)\n",
        "\n",
        "def _create_img(pixels):\n",
        "    array_pixel = str(pixels).split()    # contains list of strings\n",
        "    array = np.array(array_pixel, dtype=np.uint8)   # convert list to array(8bit int) since all pixels are unsigned ints\n",
        "    array = array.reshape((IMG_WIDTH, IMG_HEIGHT))  # reshape to make proper dimensions\n",
        "    \n",
        "    # fromarray modes ref: https://pillow.readthedocs.io/en/3.1.x/handbook/concepts.html#concept-modes\n",
        "    return Image.fromarray(array, 'L')   # create image, L means 8 bit pixels, grey\n",
        "    # img.save('my.png')\n",
        "    # img.show()\n",
        "def get_img_dir(emotion_type, img_type, index):\n",
        "    parent_dir = '' # default value\n",
        "    if str(img_type) == 'Training':\n",
        "        parent_dir = TRAIN_DIR\n",
        "        # save image inside train directory\n",
        "    else:\n",
        "        parent_dir = TEST_DIR\n",
        "        # save image inside test directory\n",
        "\n",
        "    subdir_name = Emotion[emotion_type]\n",
        "    return os.path.join(parent_dir, subdir_name + \"/\" + str(index) +\".png\")\n",
        "\n",
        "def see_data(data):\n",
        "    print(data.head) # to see the data, has 3 cols: emotion, pixels, Usage\n",
        "    # see unique values for emotion and usage column to understand what's going on\n",
        "    print(data.emotion.unique())    # 0,1,2,3,4,5,6 => 7 emotions in total\n",
        "    print(data.Usage.unique())  # Training, PublicTest, PrivateTest\n",
        "    print(data.groupby(['Usage']).size()) # PrivateTest 3589, PublicTest 3589, Training 28709\n",
        "    # From above commented code, we see 20% test data(private and public), and 80% train data.\n",
        "    \n",
        "    pixels = data.head(1).pixels   # array that contains pixel values of the first 10 rows\n",
        "    for row in pixels:\n",
        "        print(len(str(row).split()))    # 2304 pixels per row = 48*48 (split returns a list)\n",
        "\n",
        "def _make_dir():\n",
        "    ##################################################################################\n",
        "    # from kaggle dataset, the emotions are: 0=Angry, 1=Disgust, 2=Fear, 3=Happy, 4=Sad, 5=Surprise, 6=Neutral\n",
        "    # We will create directory like below:\n",
        "    # face_images/\n",
        "    #     train/\n",
        "    #         angry/\n",
        "    #         disgust/\n",
        "    #         fear/\n",
        "    #         happy/\n",
        "    #         neutral/\n",
        "    #         sad/\n",
        "    #         surprise/\n",
        "    #     test/\n",
        "    #         angry/\n",
        "    #         disgust/\n",
        "    #         fear/\n",
        "    #         happy/\n",
        "    #         neutral/\n",
        "    #         sad/\n",
        "    #         surprise/\n",
        "    ##################################################################################\n",
        "\n",
        "    current_file_path = os.path.dirname(Path().absolute()) # in colab, __file__ does not work: https://stackoverflow.com/questions/3430372/how-do-i-get-the-full-path-of-the-current-files-directory\n",
        "    base_dir = os.path.join(current_file_path, \"face_images\") # first argument is an absolute path\n",
        "    global TRAIN_DIR, TEST_DIR  # global keyword is used to make chnages to the global var\n",
        "    TRAIN_DIR = os.path.join(base_dir, \"train/\")\n",
        "    TEST_DIR = os.path.join(base_dir, \"test/\")\n",
        "    \n",
        "    # Create train directories\n",
        "    # ref for Path: https://docs.python.org/3/library/pathlib.html#pathlib.PurePath\n",
        "    # parents=true means missing parents will be added, exist_ok true means it does not throw exception if dir already exists\n",
        "    Path(TRAIN_DIR + \"angry/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TRAIN_DIR + \"disgust/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TRAIN_DIR + \"fear/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TRAIN_DIR + \"happy/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TRAIN_DIR + \"neutral/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TRAIN_DIR + \"sad/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TRAIN_DIR + \"surprise/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Create test directories\n",
        "    Path(TEST_DIR + \"angry/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TEST_DIR + \"disgust/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TEST_DIR + \"fear/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TEST_DIR + \"happy/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TEST_DIR + \"neutral/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TEST_DIR + \"sad/\").mkdir(parents=True, exist_ok=True)\n",
        "    Path(TEST_DIR + \"surprise/\").mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "##\n",
        "\n",
        "print('before creating images')\n",
        "csv_to_images('/content/drive/My Drive/dataset/fer2013.csv')  # this file exists in my google drive\n",
        "print('success!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "before creating images\n",
            "success!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HltDhv-9MdJm"
      },
      "source": [
        "# # configure kaggle api, it is not necessary in PC. \n",
        "# # For ubuntu e.g. local PC, just copy paste kaggle.json file in ~/.kaggle/\n",
        "# !mkdir ~/.kaggle\n",
        "# !touch ~/.kaggle/kaggle.json\n",
        "# api_token = {\"username\":\"akmsharifkaiser\",\"key\":\"8c9a04ae5b876fb2b196e76064a3afd6\"}\n",
        "\n",
        "# import json\n",
        "\n",
        "# with open('/root/.kaggle/kaggle.json', 'w') as file:\n",
        "#     json.dump(api_token, file)\n",
        "\n",
        "# !chmod 600 ~/.kaggle/kaggle.json\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEsgwsqbHFn2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "453a4756-9d1f-490c-b5fb-cfa33a8e6a25"
      },
      "source": [
        "# This code block does image augmentation on train images to get better result\n",
        "# augmentation like random rotation, flip, zoom etc. gives more variety to the model,\n",
        "# which helps to the fact that model is not memorizing images\n",
        "# Augmented data directory is the same as train and test directory, the model will \n",
        "# run many times and each time it will see a randomly augmented image which will \n",
        "# ease generalizing the train data\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import glob\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "IMAGE_RES = 48  # for this dataset img shape is 48*48\n",
        "# IMAGE_RES = 299 # for inception v3\n",
        "# IMAGE_RES = 224 # for mobilenet v2\n",
        "\n",
        "BASE_DIR = \"../face_images/\"\n",
        "TRAIN_DIR = os.path.join(BASE_DIR, 'train')\n",
        "TEST_DIR = os.path.join(BASE_DIR, 'test')\n",
        "\n",
        "\n",
        "# This function will plot images in the form of a grid with 1 row and 5 columns where images are placed in each column.\n",
        "def plotImages(images_arr):\n",
        "    fig, axes = plt.subplots(1, 5, figsize=(20,20))\n",
        "    axes = axes.flatten()\n",
        "    for img, ax in zip( images_arr, axes):\n",
        "        ax.imshow(img)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# image augmentation makes it worse, so no augmentation\n",
        "# image_gen_train = ImageDataGenerator(\n",
        "#   rescale=1./255, # rescaling is necessary, ref: https://www.linkedin.com/pulse/keras-image-preprocessing-scaling-pixels-training-adwin-jahn\n",
        "#   rotation_range=45,\n",
        "#   width_shift_range=0.10,\n",
        "#   height_shift_range=0.10,\n",
        "#   shear_range=0.1,\n",
        "#   horizontal_flip=True,\n",
        "#   fill_mode='nearest'\n",
        "# )\n",
        "# image_gen_val = ImageDataGenerator(rescale=1./255) \n",
        "\n",
        "# Try with no augmentation\n",
        "# image_gen_train = ImageDataGenerator(rescale=1./255)\n",
        "# image_gen_val = ImageDataGenerator(rescale=1./255)\n",
        "image_gen_train = ImageDataGenerator()\n",
        "image_gen_val = ImageDataGenerator()\n",
        "\n",
        "# Put It All Together\n",
        "train_data_gen = image_gen_train.flow_from_directory(\n",
        "  batch_size=BATCH_SIZE,\n",
        "  directory=TRAIN_DIR,\n",
        "  shuffle=True,\n",
        "  target_size=(IMAGE_RES,IMAGE_RES),\n",
        "  # color_mode='rgb', # will be converted to 3 channels, for transfer learning\n",
        "  color_mode='grayscale',\n",
        "  # class_mode='sparse' # will be 1D integer labels\n",
        "  class_mode='categorical' # for scratch training\n",
        ")\n",
        "\n",
        "augmented_images = [train_data_gen[0][0][1] for i in range(5)]\n",
        "# augmented_images = [format(train_data_gen.shape[2]) for i in range(5)]\n",
        "# plotImages(augmented_images)  # See the results\n",
        "\n",
        "# Create a Data Generator for the Validation Set\n",
        "val_data_gen = image_gen_val.flow_from_directory(\n",
        "    batch_size=BATCH_SIZE,\n",
        "    directory=TEST_DIR,\n",
        "    shuffle=False,\n",
        "    target_size=(IMAGE_RES, IMAGE_RES),\n",
        "    # color_mode='rgb', # will be converted to 3 channels\n",
        "    color_mode='grayscale',\n",
        "    # class_mode='sparse', # for transfer learning\n",
        "    class_mode='categorical' # for scratch training\n",
        "  )\n",
        "\n",
        "# print(len(list(train_data_gen)))\n",
        "# train_data_gen is a tuples of (x, y) where x is a numpy array containing a batch of images\n",
        "# with shape (batch_size, *target_size, channels) and \n",
        "# y is a numpy array of corresponding labels.\n",
        "\n",
        "# count number of train images for each expression\n",
        "for expression in os.listdir(BASE_DIR + \"train\"):\n",
        "    print(str(len(os.listdir(BASE_DIR + \"train/\" + expression))) + \" \" + expression + \" images\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 28709 images belonging to 7 classes.\n",
            "Found 7178 images belonging to 7 classes.\n",
            "3171 surprise images\n",
            "4097 fear images\n",
            "3995 angry images\n",
            "436 disgust images\n",
            "7215 happy images\n",
            "4965 neutral images\n",
            "4830 sad images\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CtFmF7A5E4tk"
      },
      "source": [
        "# Train the model with resnet50\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jg5ar6rcE4H-"
      },
      "source": [
        "import torch.nn as nn\n",
        "from torchvision.models import resnet50\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "#                               Utilities for initializing, loading, and saving models\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def init_grayscale_resnet(init_weights=None):\n",
        "    \"\"\"Initialize a pretrained Resnet-50 model and change the first layer to be a one-channel 2D convolution\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    gray_model: nn.Module\n",
        "    \"\"\"\n",
        "    gray_model = resnet50(pretrained=True)\n",
        "    if init_weights is None:\n",
        "        w = torch.zeros((64, 1, 7, 7))\n",
        "        nn.init.kaiming_uniform_(w, a=math.sqrt(5))\n",
        "    else:\n",
        "        w = init_weights\n",
        "    gray_model.conv1.weight.data = w\n",
        "    return gray_model\n",
        "\n",
        "\n",
        "def load_model(model_state, num_emotions=5):\n",
        "    \"\"\"Redefine the model architecture and load the parameter state from a saved model\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_state: str\n",
        "        The path to the saved model state dictionary\n",
        "    num_emotions: int, optional\n",
        "        The number of emotions to classify (default=5)\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    model: nn.Module\n",
        "    \"\"\"\n",
        "    model = init_grayscale_resnet()\n",
        "    conv_out_features = model.fc.in_features\n",
        "    model.fc = nn.Linear(conv_out_features, num_emotions)\n",
        "    model.load_state_dict(torch.load(model_state))\n",
        "    return model\n",
        "\n",
        "\n",
        "def checkpoint(model, filepath):\n",
        "    \"\"\"Save the state of the model\n",
        "\n",
        "    To restore the model do the following:\n",
        "    >> the_model = TheModelClass(*args, **kwargs)\n",
        "    >> the_model.load_state_dict(torch.load(PATH))\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: nn.Module\n",
        "        The pytorch model to be saved\n",
        "    filepath: str\n",
        "        The filepath of the pickle\n",
        "    \"\"\"\n",
        "    torch.save(model.state_dict(), filepath)\n",
        "\n",
        "\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "#                               Functions for training the emotion classifier model\n",
        "# ----------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def predict_label(output):\n",
        "    \"\"\"Get the index of the emotion with the highest softmax probability\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    output: torch.Tensor\n",
        "        The output of the emotion classifier (for one example, shape: [num emotions])\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    int\n",
        "    \"\"\"\n",
        "    return torch.argmax(F.softmax(output, dim=1)).item()\n",
        "\n",
        "\n",
        "def accuracy(model_out, true_labels):\n",
        "    \"\"\"Calculate the accuracy of a batch of predictions\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model_out: torch.FloatTensor\n",
        "        The output of the emotion classifier with shape [batch size, num emotions]\n",
        "    true_labels: torch.LongTensor\n",
        "        The true encoded emotion labels aligned with the model's output\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "    \"\"\"\n",
        "    pred = torch.argmax(F.softmax(model_out, dim=1), dim=1)\n",
        "    acc = (pred == true_labels.squeeze())\n",
        "    return float(acc.sum()) / acc.size(0)\n",
        "\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    \"\"\"Train the model for an epoch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: nn.Module\n",
        "    dataloader: DataLoader\n",
        "    criterion: callable loss function\n",
        "    optimizer: pytorch optimizer\n",
        "    device: torch.device\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The average loss\n",
        "    \"\"\"\n",
        "    avg_loss = []\n",
        "    model.train()\n",
        "    for batch_image, batch_label in dataloader:\n",
        "        batch_image = batch_image.to(device)\n",
        "        batch_label = batch_label.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(batch_image)\n",
        "        loss = criterion(output, batch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        avg_loss.append(loss.item())\n",
        "    return sum(avg_loss) / len(avg_loss)\n",
        "\n",
        "\n",
        "def val_epoch(model, dataloader, criterion, device):\n",
        "    \"\"\"Run the model for a validation epoch\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model: nn.Module\n",
        "    dataloader: DataLoader\n",
        "    criterion: callable loss function\n",
        "    device: torch.device\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float, float\n",
        "        The average loss and the average accuracy\n",
        "    \"\"\"\n",
        "    avg_loss = []\n",
        "    avg_acc = []\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        for batch_image, batch_label in dataloader:\n",
        "            batch_image = batch_image.to(device)\n",
        "            batch_label = batch_label.to(device)\n",
        "            output = model(batch_image)\n",
        "            loss = criterion(output, batch_label)\n",
        "            acc = accuracy(output, batch_label)\n",
        "            avg_loss.append(loss.item())\n",
        "            avg_acc.append(acc)\n",
        "    return sum(avg_loss) / len(avg_loss), sum(avg_acc) / len(avg_acc)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLxTcprUqJaq"
      },
      "source": [
        "# TODO: Plot Training and Validation Graphs\n",
        "\n",
        "In the cell below, plot the training and validation accuracy/loss graphs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d28dhbFpr98b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "outputId": "9021f391-d2ea-4dad-8843-ff616bed30ef"
      },
      "source": [
        "# plot the evolution of Loss and Acuracy on the train and validation sets\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.suptitle('Optimizer : Adam', fontsize=10)\n",
        "plt.ylabel('Loss', fontsize=16)\n",
        "plt.plot(history.history['loss'], label='Training Loss')\n",
        "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
        "plt.legend(loc='upper right')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.ylabel('Accuracy', fontsize=16)\n",
        "plt.plot(history.history['acc'], label='Training Accuracy')\n",
        "plt.plot(history.history['val_acc'], label='Validation Accuracy')\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-485bdf2952b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuptitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Optimizer : Adam'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfontsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Training Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Validation Loss'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'upper right'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'history' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAn8AAAKGCAYAAAA70/0qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niPgDABjk/wEs5G+dzQqcyAAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 1440x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5zmoDisGvNye"
      },
      "source": [
        "What is a bit curious here is that validation performance is better than training performance, right from the start to the end of execution.\n",
        "\n",
        "One reason for this is that validation performance is measured at the end of the epoch, but training performance is the average values across the epoch.\n",
        "\n",
        "The bigger reason though is that we're reusing a large part of MobileNet which is already trained on Flower images. \n",
        "\n",
        "For both mobilenet and inception, after 1 epoch, divergence happens"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kb__ZN8uFn-D"
      },
      "source": [
        "# Save model\n",
        "\n",
        "In the cell below get the label names from the `dataset info` and convert them into a NumPy array. Print the array to make sure you have the correct label names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W_Zvg2i0fzJu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "8cd2ed4d-c582-4d50-c104-4263450ec70d"
      },
      "source": [
        "# print(dataset_info.compute_dynamic_properties)\n",
        "import time\n",
        "\n",
        "t = time.time()\n",
        "export_path_keras = \"./{}.h5\".format(int(t))\n",
        "print(export_path_keras)\n",
        "\n",
        "model.save(export_path_keras)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./1580299770.h5\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-e404263e60c2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_path_keras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexport_path_keras\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Olg6MsNGJTL"
      },
      "source": [
        "### Load model\n",
        "\n",
        "In the cell below, use the `next()` function to create an `image_batch` and its corresponding `label_batch`. Convert both the `image_batch` and `label_batch` to numpy arrays using the `.numpy()` method. Then use the `.predict()` method to run the image batch through your model and make predictions. Then use the `np.argmax()` function to get the indices of the best prediction for each image. Finally convert the indices of the best predictions to class names."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fCLVCpEjJ_VP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19dc47f3-e08a-4348-bc02-631e82cd29ce"
      },
      "source": [
        "export_path_keras = '/content/drive/My Drive/models/model_weights.h5' # load from drive\n",
        "import tensorflow_hub as hub\n",
        "loaded_model = tf.keras.models.load_model(\n",
        "  export_path_keras, \n",
        "  # `custom_objects` tells keras how to load a `hub.KerasLayer`\n",
        "  custom_objects={'KerasLayer': hub.KerasLayer})\n",
        "\n",
        "loaded_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_1 (Conv2D)            (None, 48, 48, 64)        640       \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 48, 48, 64)        256       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 48, 48, 64)        0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2 (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 24, 24, 64)        0         \n",
            "_________________________________________________________________\n",
            "conv2d_2 (Conv2D)            (None, 24, 24, 128)       204928    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 24, 24, 128)       512       \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 24, 24, 128)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2 (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 12, 12, 128)       0         \n",
            "_________________________________________________________________\n",
            "conv2d_3 (Conv2D)            (None, 12, 12, 512)       590336    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 12, 12, 512)       2048      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 12, 12, 512)       0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2 (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "conv2d_4 (Conv2D)            (None, 6, 6, 512)         2359808   \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 6, 6, 512)         2048      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 6, 6, 512)         0         \n",
            "_________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2 (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 3, 3, 512)         0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 4608)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 256)               1179904   \n",
            "_________________________________________________________________\n",
            "batch_normalization_5 (Batch (None, 256)               1024      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 512)               131584    \n",
            "_________________________________________________________________\n",
            "batch_normalization_6 (Batch (None, 512)               2048      \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dropout_6 (Dropout)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 7)                 3591      \n",
            "=================================================================\n",
            "Total params: 4,478,727\n",
            "Trainable params: 4,474,759\n",
            "Non-trainable params: 3,968\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CkGbZxl9GZs-"
      },
      "source": [
        "### Prediction\n",
        "\n",
        "In the cell below, print the true labels and the indices of predicted labels."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nL9IhOmGI5dJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "c2c745b8-ef6a-43e7-f160-72da3b31d889"
      },
      "source": [
        "from PIL import Image\n",
        "import numpy as np\n",
        "from skimage import transform\n",
        "import cv2\n",
        "\n",
        "TARGET_RES = 48\n",
        "\n",
        "class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Neutral', 'Sad', 'Surprise']\n",
        "# class_names = val_data_gen.class_indices.keys()\n",
        "\n",
        "base_path = '/content/drive/My Drive/models/fer prediction google images/'\n",
        "happy_path = base_path + 'happy/'\n",
        "angry_path = base_path + 'angry/'\n",
        "disgust_path = base_path + 'disgust/'\n",
        "surprise_path = base_path + 'surprise/'\n",
        "sad_path = base_path + 'sad/'\n",
        "\n",
        "def load(filename):\n",
        "   np_image = Image.open(filename)\n",
        "   np_image = np.array(np_image).astype('float32')/255  # sample input should be float32 to match with tflite converter\n",
        "   np_image = transform.resize(np_image, (TARGET_RES, TARGET_RES, 1))\n",
        "   np_image = np.expand_dims(np_image, axis=0)\n",
        "   return np_image\n",
        "\n",
        "# def load(filename):\n",
        "#   np_image = cv2.imread(filename)\n",
        "#   np_image = np.array(np_image).astype('float32')/255\n",
        "#   np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2GRAY)\n",
        "#   roi = cv2.resize(np_image, (48, 48))\n",
        "#   img = roi[np.newaxis, :, :, np.newaxis]\n",
        "#   return img\n",
        "\n",
        "# test code below:\n",
        "happy_man = load(happy_path + 'man.jpg') # happy man\n",
        "happy_baby1 = load(happy_path + 'baby1.jpeg')\n",
        "happy_baby2 = load(happy_path + 'baby2.jpg')\n",
        "happy_woman = load(happy_path + 'woman.jpg')\n",
        "angry_man = load(angry_path + 'man.jpg')\n",
        "angry_woman = load(angry_path + 'woman.jpg')\n",
        "angry_baby = load(angry_path + 'baby.jpg')\n",
        "disgust_man = load(disgust_path + 'man.jpg')\n",
        "disgust_woman = load(disgust_path + 'woman.jpg')\n",
        "disgust_baby = load(disgust_path + 'baby.jpg')\n",
        "surprised_man = load(surprise_path + 'man.jpg')\n",
        "surprised_woman = load(surprise_path + 'woman.jpeg')\n",
        "surprised_baby = load(surprise_path + 'baby.jpg')\n",
        "sad_man = load(sad_path + 'man.jpg')\n",
        "sad_woman = load(sad_path + 'woman.jpg')\n",
        "sad_baby = load(sad_path + 'baby.jpg')\n",
        "surprise_train = load(surprise_path + '55.png')\n",
        "sad_coral = load(happy_path + 'sad80.jpg')  # from coral, it was 80% sad, here, it is 48% neutral\n",
        "\n",
        "# plt.figure()\n",
        "# plt.imshow(image6.squeeze(), cmap=plt.cm.binary)\n",
        "\n",
        "# predictions_array = loaded_model.predict(happy_man) # correct\n",
        "# predictions_array = loaded_model.predict(happy_baby2) # wrong\n",
        "# predictions_array = loaded_model.predict(angry_woman) # wrong, sad\n",
        "# predictions_array = loaded_model.predict(disgust_man) # wrong, sad\n",
        "# predictions_array = loaded_model.predict(surprised_man) # correct\n",
        "# predictions_array = loaded_model.predict(surprised_baby) # wrong\n",
        "# predictions_array = loaded_model.predict(sad_man) # wrong, shows neutral\n",
        "# predictions_array = loaded_model.predict(sad_baby) # correct\n",
        "# predictions_array = loaded_model.predict(surprise_train)\n",
        "predictions_array = loaded_model.predict(happy_baby1)\n",
        "\n",
        "confidence = f\"{100*np.max(predictions_array):.2f}\"\n",
        "detected_emotion = class_names[np.argmax(predictions_array)]\n",
        "emotion_with_confidence_output = detected_emotion + ': ' + confidence + '%'\n",
        "\n",
        "print(predictions_array, emotion_with_confidence_output)\n",
        "# print(image5.shape)\n",
        "\n",
        "# predicted_batch = tf.squeeze(predicted_batch) # has (32,2) shape\n",
        "\n",
        "# axis 0 means rows, axis 1 means cols, axis -1 includes all values\n",
        "# predicted_ids = np.argmax(predicted_batch, axis=1)\n",
        "# predicted_class_names = class_names[predicted_ids]\n",
        "# predicted_class_names"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[7.2155730e-05 1.9262693e-08 2.3543891e-04 9.9740666e-01 1.2597096e-03\n",
            "  3.5820549e-04 6.6781614e-04]] Happy: 99.74%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJDyzEfYuFcW"
      },
      "source": [
        "# Save model and export"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wC_AYRJU9NQe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "f3a03c70-64df-4a68-b859-e974f8285c7f"
      },
      "source": [
        "# We don't need to convert to pb as we found another way to convert directly from .h5 file\n",
        "# export_dir = \"exp_saved_model\" # export directory for saving\n",
        "\n",
        "# # saves the model to RPS_SAVED_MODEL directory as .pb file \n",
        "# tf.saved_model.save(loaded_model, export_dir)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1781: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "INFO:tensorflow:Assets written to: exp_saved_model/assets\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRi01Yr4195R",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "24797675-b3fa-4314-a8b9-002b2ff78923"
      },
      "source": [
        "# https://stackoverflow.com/questions/57877959/what-is-the-correct-way-to-create-representative-dataset-for-tfliteconverter\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def load_img_tflite(filename):\n",
        "  np_image = cv2.imread(filename)\n",
        "  np_image = np.array(np_image).astype('float32')/255\n",
        "  np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2GRAY)\n",
        "  roi = cv2.resize(np_image, (48, 48))\n",
        "  img = roi[np.newaxis, :, :, np.newaxis]\n",
        "  return img\n",
        "\n",
        "def representative_dataset_gen():\n",
        "  loaded_image = load_img_tflite(happy_path + 'man.jpg')\n",
        "  img = tf.data.Dataset.from_tensor_slices(loaded_image).batch(1)  # convert to iterable tensor slice\n",
        "  # print(img)\n",
        "  # num_calibration_steps = 10\n",
        "  for i in img.take(BATCH_SIZE):\n",
        "    yield [i]\n",
        "\n",
        "\n",
        "# converter = tf.lite.TFLiteConverter.from_saved_model(export_dir)\n",
        "\n",
        "# using keras model converter, directly from .h5 file\n",
        "converter  = tf.lite.TFLiteConverter.from_keras_model_file('/content/drive/My Drive/models/model_weights.h5')\n",
        "# converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS, tf.lite.OpsSet.SELECT_TF_OPS]\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.representative_dataset = representative_dataset_gen\n",
        "\n",
        "# following code is for edgetpu support full int quantization\n",
        "converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
        "converter.inference_input_type = tf.uint8\n",
        "converter.inference_output_type = tf.uint8\n",
        "\n",
        "tflite_quant_model = converter.convert()\n",
        "\n",
        "open(\"fer.tflite\", \"wb\").write(tflite_quant_model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4511296"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UnX3ySSdiozU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "outputId": "94a3c7da-be03-4cac-df50-5fe6803614ca"
      },
      "source": [
        "# Run tflite inference for testing\n",
        "import numpy as np\n",
        "import cv2\n",
        "\n",
        "def load_img_tflite(filename):\n",
        "  np_image = cv2.imread(filename)\n",
        "  np_image = np.asarray(np_image, dtype=\"float32\" )/255\n",
        "  np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2GRAY)\n",
        "  cvuint8 = cv2.convertScaleAbs(np_image)\n",
        "  roi = cv2.resize(cvuint8, (48, 48))\n",
        "  img = roi[np.newaxis, :, :, np.newaxis]\n",
        "  return img\n",
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "interpreter = tf.lite.Interpreter(model_path=\"/content/drive/My Drive/models/fer1.tflite\")\n",
        "\n",
        "\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# first allocate_tensors and then input, output\n",
        "# https://www.tensorflow.org/lite/guide/inference\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_index = input_details[0][\"index\"]\n",
        "output_index = output_details[0][\"index\"]\n",
        "\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "print (input_details, output_details)\n",
        "\n",
        "# Gather results for the randomly sampled test images\n",
        "# predictions = []\n",
        "test_labels = []\n",
        "test_images = []\n",
        "\n",
        "sad_coral = load_img_tflite(happy_path + 'sad80.jpg')  # from coral, it was 80% sad, here, it is 48% neutral\n",
        "\n",
        "\n",
        "# ref: https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter\n",
        "interpreter.set_tensor(input_index, sad_coral)  # sad_coral is actual input image\n",
        "interpreter.invoke()  # this invokes the inference\n",
        "predictions = interpreter.get_tensor(output_index)\n",
        "# test_labels.append(label[0])\n",
        "# test_images.append(np.array(img))\n",
        "\n",
        "# Get input and output tensors.\n",
        "# input_details = interpreter.get_input_details()\n",
        "# output_details = interpreter.get_output_details()\n",
        "print(predictions)\n",
        "print(predictions.argmax())\n",
        "print(predictions.max())\n",
        "# print(str(np.argmax(predictions[0])))\n",
        "# confidence = f\"{100*np.max(predictions[0]):.2f}\"\n",
        "# print(confidence)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[{'name': 'conv2d_1_input', 'index': 27, 'shape': array([ 1, 48, 48,  1], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.003730571595951915, 0)}] [{'name': 'Identity', 'index': 28, 'shape': array([1, 7], dtype=int32), 'dtype': <class 'numpy.uint8'>, 'quantization': (0.00390625, 0)}]\n",
            "[[122   1  21   5  25  58  23]]\n",
            "0\n",
            "122\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "npR2vjsv4lBy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        },
        "outputId": "e37ab84a-77ef-4fa9-9b80-6988cd91e19b"
      },
      "source": [
        "# !sudo apt-get update\n",
        "# !sudo apt install libedgetpu1-std # edgetpu runtime installation\n",
        "!sudo apt-get install edgetpu\n",
        "# !python3 -m pip install libedgetpu1-std\n",
        "!lsb_release -a  # check ubuntu version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "E: Unable to locate package edgetpu\n",
            "No LSB modules are available.\n",
            "Distributor ID:\tUbuntu\n",
            "Description:\tUbuntu 18.04.3 LTS\n",
            "Release:\t18.04\n",
            "Codename:\tbionic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WsTeIdT-5AAB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        },
        "outputId": "da16d8c5-086e-4f39-fd89-41c034b186ec"
      },
      "source": [
        "# install tflite runtime\n",
        "!pip3 install https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp36-cp36m-linux_x86_64.whl\n",
        "# !python --version"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tflite-runtime==2.1.0\n",
            "\u001b[?25l  Downloading https://dl.google.com/coral/python/tflite_runtime-2.1.0-cp36-cp36m-linux_x86_64.whl (1.9MB)\n",
            "\u001b[K     |████████████████████████████████| 1.9MB 1.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12.1 in /usr/local/lib/python3.6/dist-packages (from tflite-runtime==2.1.0) (1.17.5)\n",
            "Installing collected packages: tflite-runtime\n",
            "Successfully installed tflite-runtime-2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2rHzvJtg6kz4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "b9d76ccf-3c3d-429f-9b0d-b009e873cac6"
      },
      "source": [
        "# This is for testing tflite edgetpu version\n",
        "import numpy as np\n",
        "import cv2\n",
        "import tflite_runtime.interpreter as tflite\n",
        "\n",
        "def load_img_tflite(filename):\n",
        "  np_image = cv2.imread(filename)\n",
        "  np_image = cv2.cvtColor(np_image, cv2.COLOR_BGR2GRAY)  \n",
        "  roi = cv2.resize(np_image, (48, 48))\n",
        "  img = roi[np.newaxis, :, :, np.newaxis]\n",
        "  return img\n",
        "\n",
        "# Load TFLite model and allocate tensors.\n",
        "model_path=\"/content/drive/My Drive/models/fer_edgetpu.tflite\"\n",
        "interpreter = tflite.Interpreter(model_path, experimental_delegates=[tflite.load_delegate('libedgetpu.so.1')])\n",
        "interpreter.allocate_tensors()\n",
        "\n",
        "# first allocate_tensors and then input, output\n",
        "# https://www.tensorflow.org/lite/guide/inference\n",
        "input_details = interpreter.get_input_details()\n",
        "output_details = interpreter.get_output_details()\n",
        "input_index = input_details[0][\"index\"]\n",
        "output_index = output_details[0][\"index\"]\n",
        "\n",
        "input_shape = input_details[0]['shape']\n",
        "\n",
        "print (input_details, output_details)\n",
        "\n",
        "# Gather results for the randomly sampled test images\n",
        "predictions = []\n",
        "test_labels = []\n",
        "test_images = []\n",
        "\n",
        "sad_coral = load_img_tflite(happy_path + 'sad80.jpg')  # from coral, it was 80% sad, here, it is 48% neutral\n",
        "\n",
        "\n",
        "# ref: https://www.tensorflow.org/api_docs/python/tf/lite/Interpreter\n",
        "interpreter.set_tensor(input_index, sad_coral)  # sad_coral is actual input image\n",
        "interpreter.invoke()  # this invokes the inference\n",
        "predictions.append(interpreter.get_tensor(output_index))\n",
        "# test_labels.append(label[0])\n",
        "# test_images.append(np.array(img))\n",
        "\n",
        "# Get input and output tensors.\n",
        "# input_details = interpreter.get_input_details()\n",
        "# output_details = interpreter.get_output_details()\n",
        "print(predictions)\n",
        "print(str(np.argmax(predictions[0])))\n",
        "confidence = f\"{100*np.max(predictions[0]):.2f}\"\n",
        "print(confidence)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-57-91df6465f1c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Load TFLite model and allocate tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mmodel_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"/content/drive/My Drive/models/fer_edgetpu.tflite\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0minterpreter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtflite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInterpreter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperimental_delegates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtflite\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_delegate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'libedgetpu.so.1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0minterpreter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallocate_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tflite_runtime/interpreter.py\u001b[0m in \u001b[0;36mload_delegate\u001b[0;34m(library, options)\u001b[0m\n\u001b[1;32m    159\u001b[0m   \"\"\"\n\u001b[1;32m    160\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m     \u001b[0mdelegate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDelegate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     raise ValueError('Failed to load delegate from {}\\n{}'.format(\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tflite_runtime/interpreter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, library, options)\u001b[0m\n\u001b[1;32m     88\u001b[0m                          'due to missing immediate reference counting.')\n\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 90\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_library\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpydll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlibrary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m     self._library.tflite_plugin_create_delegate.argtypes = [\n\u001b[1;32m     92\u001b[0m         \u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOINTER\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_char_p\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ctypes/__init__.py\u001b[0m in \u001b[0;36mLoadLibrary\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mLoadLibrary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dlltype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[0mcdll\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLibraryLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mCDLL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/ctypes/__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, name, mode, handle, use_errno, use_last_error)\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 348\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_dlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    349\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: libedgetpu.so.1: cannot open shared object file: No such file or directory"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y82J-xJou1M1"
      },
      "source": [
        "# show the confusion matrix of our predictions\n",
        "\n",
        "# compute predictions\n",
        "predictions = model.predict_generator(generator=validation_generator)\n",
        "y_pred = [np.argmax(probas) for probas in predictions]\n",
        "y_test = validation_generator.classes\n",
        "class_names = validation_generator.class_indices.keys()\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import itertools\n",
        "\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    plt.figure(figsize=(10,10))\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "\n",
        "    fmt = '.2f'\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, format(cm[i, j], fmt),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "    plt.tight_layout()\n",
        "    \n",
        "# compute confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "np.set_printoptions(precision=2)\n",
        "\n",
        "# plot normalized confusion matrix\n",
        "plt.figure()\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Normalized confusion matrix')\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}